{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a305ee48-66e8-464c-8dc9-45bf3cfb159c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed50e3c-b7e2-46fc-b2c9-491473aec10f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b99e9a-3f06-4418-8fe4-c967481aeab0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# cett ø3\n",
    "role = sagemaker.get_execution_role()\n",
    "sess= sagemaker.Session()\n",
    "\n",
    "region=boto3.Session().region_name\n",
    "\n",
    "# S3 bucket for training data.\n",
    "# this witt create bucket tike Accountld>'\n",
    "data_bucket=sess.default_bucket()\n",
    "data_prefix = \"lp—notebooks-datasets/taxi/text—csv\"\n",
    "# S3 bucket for saving code and modet artifacts.\n",
    "output_bucket = data_bucket\n",
    "output_prefix =\"sagemaker/DEMO-linear-learner-taxifare-regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0666ea0c-d8ee-4382-9316-f1b6d9ce2dee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# file_train=\"ecommerce_customer_data_large.csv\"\n",
    "# import pandas as pd\n",
    "\n",
    "# df=pd.read_csv(file_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "b4e9401f-fe06-443d-a8ed-4f9e4de0226c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"LocationConstraint\": null\n",
      "}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad69f15-44a4-4fba-a96a-f7184a87c339",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c850fa-0c76-4cfe-8e8e-de964d2a9ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e53e3-7a45-42af-a803-737b137789b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7672a366-5b76-4068-8257-0f542cb61f80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://deb.debian.org/debian bullseye InRelease\n",
      "Hit:2 http://security.debian.org/debian-security bullseye-security InRelease\n",
      "Ign:3 http://security.debian.org/debian-security stretch/updates InRelease\n",
      "Hit:4 http://deb.debian.org/debian bullseye-updates InRelease\n",
      "Err:5 http://security.debian.org/debian-security stretch/updates Release\n",
      "  404  Not Found [IP: 151.101.130.132 80]\n",
      "Reading package lists... Done\n",
      "E: The repository 'http://security.debian.org/debian-security stretch/updates Release' does not have a Release file.\n",
      "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
      "N: See apt-secure(8) manpage for repository creation and user configuration details.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "software-properties-common is already the newest version (0.96.20.2-2.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "default-jdk is already the newest version (2:1.11-72).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
      "Hit:1 http://security.debian.org/debian-security bullseye-security InRelease\n",
      "Hit:2 http://deb.debian.org/debian bullseye InRelease\n",
      "Hit:3 http://deb.debian.org/debian bullseye-updates InRelease\n",
      "Ign:4 http://security.debian.org/debian-security stretch/updates InRelease\n",
      "Err:5 http://security.debian.org/debian-security stretch/updates Release\n",
      "  404  Not Found [IP: 151.101.66.132 80]\n",
      "Reading package lists... Done\n",
      "E: The repository 'http://security.debian.org/debian-security stretch/updates Release' does not have a Release file.\n",
      "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
      "N: See apt-secure(8) manpage for repository creation and user configuration details.\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/geerlingguy/ansible-role-java/issues/64\n",
    "!mkdir -p /usr/share/man/man1\n",
    "\n",
    "# https://stackoverflow.com/a/61902164/4281353\n",
    "# !apt update -y\n",
    "!apt-get update -y\n",
    "!apt install software-properties-common -y\n",
    "# !apt-add-repository 'deb http://security.debian.org/debian-security stretch/updates main'\n",
    "!apt install default-jdk -y\n",
    "# !apt install openjdk-8-jdk -y\n",
    "!apt-get -y update && apt-get -y upgrade\n",
    "# !apt-get install -y openjdk-8-jdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c5571c2-9e37-4934-a304-7b3bde99c121",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk 11.0.21 2023-10-17\n",
      "OpenJDK Runtime Environment (build 11.0.21+9-post-Debian-1deb11u1)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.21+9-post-Debian-1deb11u1, mixed mode, sharing)\n",
      "default-java  java-1.11.0-openjdk-amd64  java-11-openjdk-amd64\topenjdk-11\n"
     ]
    }
   ],
   "source": [
    "!java --version\n",
    "!ls '/usr/lib/jvm/'\n",
    "# %env JAVA_HOME='/usr/lib/jvm/java-1.8.0-openjdk-amd64'\n",
    "\n",
    "# %env JAVA_HOME='/usr/lib/jvm/java-11-openjdk-amd64'\n",
    "\n",
    "# import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a9b8813-c2c6-4dee-9f93-5d3424a51de7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/lib/jvm/java-11-openjdk-amd64'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !wget maven/org.apache.spark/spark-hadoop-cloud_2.12@3.5.0\n",
    "os.environ['JAVA_HOME']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f76a37a4-4d39-465a-8640-43b93930d8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install sagemaker_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64e15868-f7c8-4d48-8346-5d75a341a5f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker_pyspark.algorithms import KMeansSageMakerEstimator\n",
    "from sagemaker_pyspark import SageMakerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e104d97-b15c-4866-9ef5-cc01fde1a9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "084e215f-42ad-4f97-bf3f-66a513d494ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !export PYSPARK_SUBMIT_ARGS=\"--master local[3] pyspark-shell\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87d314a5-9e4a-4eae-b8e9-3f7d1d95cd47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !export PYSPARK_SUBMIT_ARGS=\"--master local[2] pyspark-shell\"\n",
    "# !export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home\n",
    "!export JAVA_HOME=os.environ['JAVA_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc557aef-dd8d-4476-a7d5-019d580af502",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.amazonaws#aws-java-sdk-s3 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-defcd7d6-3c66-4f82-b3ff-f45c0ca522b2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.196 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.196 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.196 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.6.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.6 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.6 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.12.6 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in central\n",
      "\tfound com.amazonaws#jmespath-java;1.12.196 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 1529ms :: artifacts dl 135ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.196 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.196 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.196 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.196 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.6 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.6 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.6.1 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.12.6 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.6 by [com.fasterxml.jackson.core#jackson-databind;2.12.6.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   20  |   0   |   0   |   3   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-defcd7d6-3c66-4f82-b3ff-f45c0ca522b2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/34ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/26 20:06:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----------------+-------------+--------+---------------------+--------------+------------+-------+--------------+---+------+-----+\n",
      "|Customer ID|      Purchase Date|Product Category|Product Price|Quantity|Total Purchase Amount|Payment Method|Customer Age|Returns| Customer Name|Age|Gender|Churn|\n",
      "+-----------+-------------------+----------------+-------------+--------+---------------------+--------------+------------+-------+--------------+---+------+-----+\n",
      "|      44605|2023-05-03 21:30:02|            Home|          177|       1|                 2427|        PayPal|          31|    1.0|   John Rivera| 31|Female|    0|\n",
      "|      44605|2021-05-16 13:57:44|     Electronics|          174|       3|                 2448|        PayPal|          31|    1.0|   John Rivera| 31|Female|    0|\n",
      "|      44605|2020-07-13 06:16:57|           Books|          413|       1|                 2345|   Credit Card|          31|    1.0|   John Rivera| 31|Female|    0|\n",
      "|      44605|2023-01-17 13:14:36|     Electronics|          396|       3|                  937|          Cash|          31|    0.0|   John Rivera| 31|Female|    0|\n",
      "|      44605|2021-05-01 11:29:27|           Books|          259|       4|                 2598|        PayPal|          31|    1.0|   John Rivera| 31|Female|    0|\n",
      "|      13738|2022-08-25 06:48:33|            Home|          191|       3|                 3722|   Credit Card|          27|    1.0|Lauren Johnson| 27|Female|    0|\n",
      "|      13738|2023-07-25 05:17:24|     Electronics|          205|       1|                 2773|   Credit Card|          27|   null|Lauren Johnson| 27|Female|    0|\n",
      "|      13738|2023-02-05 19:31:48|           Books|          370|       5|                 1486|          Cash|          27|    1.0|Lauren Johnson| 27|Female|    0|\n",
      "|      13738|2021-12-21 03:29:05|            Home|           12|       2|                 2175|          Cash|          27|   null|Lauren Johnson| 27|Female|    0|\n",
      "|      13738|2023-02-09 00:53:14|     Electronics|           40|       4|                 4327|          Cash|          27|    0.0|Lauren Johnson| 27|Female|    0|\n",
      "|      33969|2023-02-28 19:58:23|        Clothing|          410|       3|                 5018|   Credit Card|          27|   null|   Carol Allen| 27|  Male|    0|\n",
      "|      33969|2023-01-05 11:15:27|            Home|          304|       1|                 3883|        PayPal|          27|    1.0|   Carol Allen| 27|  Male|    0|\n",
      "|      33969|2023-07-18 23:36:50|           Books|           54|       2|                 4187|        PayPal|          27|    0.0|   Carol Allen| 27|  Male|    0|\n",
      "|      33969|2021-12-20 23:44:57|     Electronics|          428|       4|                 2289|          Cash|          27|    0.0|   Carol Allen| 27|  Male|    0|\n",
      "|      33969|2020-03-07 21:31:35|           Books|          281|       1|                 3810|          Cash|          27|    0.0|   Carol Allen| 27|  Male|    0|\n",
      "|      33969|2022-07-21 04:25:44|            Home|          193|       2|                 3198|   Credit Card|          27|    0.0|   Carol Allen| 27|  Male|    0|\n",
      "|      33969|2023-07-05 15:01:04|        Clothing|          473|       3|                 2881|   Credit Card|          27|    1.0|   Carol Allen| 27|  Male|    0|\n",
      "|      42650|2020-10-18 23:38:52|           Books|          127|       5|                 3347|          Cash|          20|    0.0|  Curtis Smith| 20|Female|    0|\n",
      "|      42650|2020-05-17 17:02:36|            Home|          284|       2|                 3531|   Credit Card|          20|    1.0|  Curtis Smith| 20|Female|    0|\n",
      "|      42650|2022-03-18 13:52:08|     Electronics|          256|       2|                 3548|   Credit Card|          20|    0.0|  Curtis Smith| 20|Female|    0|\n",
      "+-----------+-------------------+----------------+-------------+--------+---------------------+--------------+------------+-------+--------------+---+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# import org.apache.spark.sql.SparkSession\n",
    "import sagemaker_pyspark\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.2 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-s3:1.12.196,org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell'\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "role = get_execution_role()\n",
    "\n",
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "jars = sagemaker_pyspark.classpath_jars()\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import sagemaker_pyspark\n",
    "import botocore.session\n",
    "\n",
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder.config(\"spark.driver.extraClassPath\", classpath)\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "hadoopConf=spark._jsc.hadoopConfiguration()\n",
    "hadoopConf.set('fs.s3a.access.key','AKIA5BL67OKLIAAI6ENY')\n",
    "hadoopConf.set('fs.s3a.secret.key','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')\n",
    "hadoopConf.set('spark.haddop.fs.s3a.aws.credentials.provider','org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "df = spark.read.csv(\"ecommerce_customer_data_large.csv\", header=True, inferSchema=True, encoding='utf-8')\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60c36d9e-c52f-4c22-baa4-eb5475bd667e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6793763-2122-4e33-a26c-da512efd4d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIAX7RMTG6KERGGBYZW\")\n",
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"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\")\n",
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "# spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider\")\n",
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"eu-west-3.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea1fbc-2bb3-4fdd-8bad-390865a35bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cb002fc-4bee-489b-80ea-6c3f9fadf3f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+--------+---------------------+--------------+-------+---+------+-----+\n",
      "|Product Category|Product Price|Quantity|Total Purchase Amount|Payment Method|Returns|Age|Gender|Churn|\n",
      "+----------------+-------------+--------+---------------------+--------------+-------+---+------+-----+\n",
      "|            Home|          177|       1|                 2427|        PayPal|    1.0| 31|Female|    0|\n",
      "|     Electronics|          174|       3|                 2448|        PayPal|    1.0| 31|Female|    0|\n",
      "|           Books|          413|       1|                 2345|   Credit Card|    1.0| 31|Female|    0|\n",
      "|     Electronics|          396|       3|                  937|          Cash|    0.0| 31|Female|    0|\n",
      "|           Books|          259|       4|                 2598|        PayPal|    1.0| 31|Female|    0|\n",
      "|            Home|          191|       3|                 3722|   Credit Card|    1.0| 27|Female|    0|\n",
      "|     Electronics|          205|       1|                 2773|   Credit Card|   null| 27|Female|    0|\n",
      "|           Books|          370|       5|                 1486|          Cash|    1.0| 27|Female|    0|\n",
      "|            Home|           12|       2|                 2175|          Cash|   null| 27|Female|    0|\n",
      "|     Electronics|           40|       4|                 4327|          Cash|    0.0| 27|Female|    0|\n",
      "|        Clothing|          410|       3|                 5018|   Credit Card|   null| 27|  Male|    0|\n",
      "|            Home|          304|       1|                 3883|        PayPal|    1.0| 27|  Male|    0|\n",
      "|           Books|           54|       2|                 4187|        PayPal|    0.0| 27|  Male|    0|\n",
      "|     Electronics|          428|       4|                 2289|          Cash|    0.0| 27|  Male|    0|\n",
      "|           Books|          281|       1|                 3810|          Cash|    0.0| 27|  Male|    0|\n",
      "|            Home|          193|       2|                 3198|   Credit Card|    0.0| 27|  Male|    0|\n",
      "|        Clothing|          473|       3|                 2881|   Credit Card|    1.0| 27|  Male|    0|\n",
      "|           Books|          127|       5|                 3347|          Cash|    0.0| 20|Female|    0|\n",
      "|            Home|          284|       2|                 3531|   Credit Card|    1.0| 20|Female|    0|\n",
      "|     Electronics|          256|       2|                 3548|   Credit Card|    0.0| 20|Female|    0|\n",
      "+----------------+-------------+--------+---------------------+--------------+-------+---+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=df.drop(*['Customer ID','Customer Name','Customer Age','Purchase Date'])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4dff1e1-c1a3-461b-8dcf-b8248e05f455",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-------------+--------+---------------------+--------------+-------+------+------+------+\n",
      "|summary|Product Category|Product Price|Quantity|Total Purchase Amount|Payment Method|Returns|   Age|Gender| Churn|\n",
      "+-------+----------------+-------------+--------+---------------------+--------------+-------+------+------+------+\n",
      "|  count|          250000|       250000|  250000|               250000|        250000| 202618|250000|250000|250000|\n",
      "+-------+----------------+-------------+--------+---------------------+--------------+-------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_df=df1.describe()\n",
    "\n",
    "summary_df.filter(summary_df[\"summary\"]==\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4118115e-b952-4fe6-bd4c-5bb5d69d9a58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-------------+--------+---------------------+--------------+-------+------+------+------+\n",
      "|summary|Product Category|Product Price|Quantity|Total Purchase Amount|Payment Method|Returns|   Age|Gender| Churn|\n",
      "+-------+----------------+-------------+--------+---------------------+--------------+-------+------+------+------+\n",
      "|  count|          250000|       250000|  250000|               250000|        250000| 202618|250000|250000|250000|\n",
      "+-------+----------------+-------------+--------+---------------------+--------------+-------+------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "null_counts = df1.select([col(c).alias(c) for c in df1.columns]).summary(\"count\")\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7ad4301-7f89-4772-bc8a-e27be9c244f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = df1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02570b33-e8e1-41ca-b36f-0d2a6825d6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------------------+-------+---+----------------------+-------------------------+----------------------------+---------------------+-------------+-----------+-------------------+--------------------------+---------------------+-----+\n",
      "|Product Price|Quantity|Total Purchase Amount|Returns|Age|Product Category_Books|Product Category_Clothing|Product Category_Electronics|Product Category_Home|Gender_Female|Gender_Male|Payment Method_Cash|Payment Method_Credit Card|Payment Method_PayPal|Churn|\n",
      "+-------------+--------+---------------------+-------+---+----------------------+-------------------------+----------------------------+---------------------+-------------+-----------+-------------------+--------------------------+---------------------+-----+\n",
      "|177          |1       |2427                 |1.0    |31 |0                     |0                        |0                           |1                    |1            |0          |0                  |0                         |1                    |0    |\n",
      "|174          |3       |2448                 |1.0    |31 |0                     |0                        |1                           |0                    |1            |0          |0                  |0                         |1                    |0    |\n",
      "|413          |1       |2345                 |1.0    |31 |1                     |0                        |0                           |0                    |1            |0          |0                  |1                         |0                    |0    |\n",
      "|396          |3       |937                  |0.0    |31 |0                     |0                        |1                           |0                    |1            |0          |1                  |0                         |0                    |0    |\n",
      "|259          |4       |2598                 |1.0    |31 |1                     |0                        |0                           |0                    |1            |0          |0                  |0                         |1                    |0    |\n",
      "|191          |3       |3722                 |1.0    |27 |0                     |0                        |0                           |1                    |1            |0          |0                  |1                         |0                    |0    |\n",
      "|205          |1       |2773                 |0.0    |27 |0                     |0                        |1                           |0                    |1            |0          |0                  |1                         |0                    |0    |\n",
      "|370          |5       |1486                 |1.0    |27 |1                     |0                        |0                           |0                    |1            |0          |1                  |0                         |0                    |0    |\n",
      "|12           |2       |2175                 |0.0    |27 |0                     |0                        |0                           |1                    |1            |0          |1                  |0                         |0                    |0    |\n",
      "|40           |4       |4327                 |0.0    |27 |0                     |0                        |1                           |0                    |1            |0          |1                  |0                         |0                    |0    |\n",
      "|410          |3       |5018                 |0.0    |27 |0                     |1                        |0                           |0                    |0            |1          |0                  |1                         |0                    |0    |\n",
      "|304          |1       |3883                 |1.0    |27 |0                     |0                        |0                           |1                    |0            |1          |0                  |0                         |1                    |0    |\n",
      "|54           |2       |4187                 |0.0    |27 |1                     |0                        |0                           |0                    |0            |1          |0                  |0                         |1                    |0    |\n",
      "|428          |4       |2289                 |0.0    |27 |0                     |0                        |1                           |0                    |0            |1          |1                  |0                         |0                    |0    |\n",
      "|281          |1       |3810                 |0.0    |27 |1                     |0                        |0                           |0                    |0            |1          |1                  |0                         |0                    |0    |\n",
      "|193          |2       |3198                 |0.0    |27 |0                     |0                        |0                           |1                    |0            |1          |0                  |1                         |0                    |0    |\n",
      "|473          |3       |2881                 |1.0    |27 |0                     |1                        |0                           |0                    |0            |1          |0                  |1                         |0                    |0    |\n",
      "|127          |5       |3347                 |0.0    |20 |1                     |0                        |0                           |0                    |1            |0          |1                  |0                         |0                    |0    |\n",
      "|284          |2       |3531                 |1.0    |20 |0                     |0                        |0                           |1                    |1            |0          |0                  |1                         |0                    |0    |\n",
      "|256          |2       |3548                 |0.0    |20 |0                     |0                        |1                           |0                    |1            |0          |0                  |1                         |0                    |0    |\n",
      "+-------------+--------+---------------------+-------+---+----------------------+-------------------------+----------------------------+---------------------+-------------+-----------+-------------------+--------------------------+---------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import IntegerType\n",
    "import numpy as np\n",
    "\n",
    "prod_categories = df1.select('Product Category').distinct().rdd.flatMap(lambda x : x).collect()\n",
    "gender_categories = df1.select('Gender').distinct().rdd.flatMap(lambda x : x).collect()\n",
    "payment_categories = df1.select('Payment Method').distinct().rdd.flatMap(lambda x : x).collect()\n",
    "prod_categories.sort()\n",
    "gender_categories.sort()\n",
    "payment_categories.sort()\n",
    "for category in prod_categories:\n",
    "    function = udf(lambda item: 1 if item == category else 0, IntegerType())\n",
    "    new_column_name = 'Product Category'+'_'+category\n",
    "    df1 = df1.withColumn(new_column_name, function(col('Product Category')))\n",
    "\n",
    "for category in gender_categories:\n",
    "    \\\n",
    "    function = udf(lambda item: 1 if item == category else 0, IntegerType())\n",
    "    new_column_name = 'Gender'+'_'+category\n",
    "    df1 = df1.withColumn(new_column_name, function(col('Gender')))\n",
    "\n",
    "for category in payment_categories:\n",
    "    function = udf(lambda item: 1 if item == category else 0, IntegerType())\n",
    "    new_column_name = 'Payment Method'+'_'+category\n",
    "    df1 = df1.withColumn(new_column_name, function(col('Payment Method')))\n",
    "df1=df1.drop(*['Product Category','Payment Method','Gender'])\n",
    "df1_cols=df1.drop(\"Churn\").columns\n",
    "df1_cols=np.array(df1_cols)\n",
    "df1_cols = np.append(df1_cols, \"Churn\")\n",
    "df1_cols=list(df1_cols)\n",
    "df1 = df1.selectExpr(*[f\"`{col}`\" for col in df1_cols])\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c03c2c92-9779-4d07-873e-0cd1422377a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce88d626-34c5-4d9e-bb11-64f9aa99ccaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "# 53 bucket for training data.\n",
    "# this will create bucket like 'Sagemaker-eregions-eYour Accountids！\n",
    "data_bucket=sess.default_bucket()\n",
    "data_prefix = \"lp-notebooks-datasets/ecom/text-csv\"\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "output_bucket =data_bucket\n",
    "output_prefix = \"sagemaker/DEMO-linear-learner-ecom-regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fe9404d-1b93-43e9-b010-3115dad8d4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-896303854230'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8940cc03-3c07-467d-9b1f-8f30948c0d17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-896303854230/sagemaker/DEMO-linear-learner-ecom-regression/output'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_location=f\"s3://{output_bucket}/{output_prefix}/output\"\n",
    "output_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "fee91d7a-a7fc-4ba7-99c5-ae421e047021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# s3a://896303854230-sagemaker-us-east-1/02c182a4-14ca-4626-b678-50054e015ced/sparksm-4-trainingJob-240ac9e1cb66-2023-11-26T17-37-47-922427."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5beed4-56de-4478-ae48-3d4e554ed7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import boto3\n",
    "\n",
    "# # Create an S3 client\n",
    "# s3 = boto3.client('s3')\n",
    "\n",
    "# # List all S3 buckets\n",
    "# response = s3.list_buckets()\n",
    "\n",
    "# # Print bucket names\n",
    "# for bucket in response['Buckets']:\n",
    "#     print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7679d46b-2965-4eba-852b-b3a544837df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae19e32-f431-4411-8e14-bf70de4d9f48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pyspark\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.2 pyspark-shell'\n",
    "# from pyspark.sql import SQLContext\n",
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext()\n",
    "# sqlContext = SQLContext(sc)\n",
    "# # filePath = \"s3a://yourBucket/yourFile.parquet\"\n",
    "# # df = sqlContext.read.parquet(filePath) # Parquet file read example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "3a6ce471-bc2d-4339-a769-bae1bbadbc19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "columns_to_convert = [\n",
    "    \"Product Price\", \"Quantity\", \"Total Purchase Amount\", \"Returns\", \"Age\",\n",
    "    \"Product Category_Books\", \"Product Category_Clothing\", \"Product Category_Electronics\",\n",
    "    \"Product Category_Home\", \"Gender_Female\", \"Gender_Male\",\n",
    "    \"Payment Method_Cash\", \"Payment Method_Credit Card\", \"Payment Method_PayPal\", \"Churn\"\n",
    "]\n",
    "shuffled_df = df1.sample(fraction=1.0, seed=1729)\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    shuffled_df = shuffled_df.withColumn(column, col(column).cast(\"double\"))\n",
    "    \n",
    "\n",
    "\n",
    "# Use randomSplit with weights representing the fraction of each split\n",
    "train_data, validation_data, test_data = shuffled_df.randomSplit([0.7, 0.2, 0.1], seed=1729)\n",
    "\n",
    "# train_data = train_data.toPandas()\n",
    "# train_data=train_data.astype('double')\n",
    "    \n",
    "# validation_data = validation_data.toPandas()\n",
    "# test_data = test_data.toPandas()\n",
    "\n",
    "# train_data.to_csv('train.csv',header=True,index=False)\n",
    "# validation_data.to_csv('validation.csv',header=True,index=False)\n",
    "# test_data.to_csv('test.csv',header=False,index=False)\n",
    "# train_data.write.csv('s3://sagemaker-us-east-1-548775606164/train_data', header=True, mode='overwrite')\n",
    "# validation_data.write.csv('s3://sagemaker-us-east-1-548775606164/validation_data', header=True, mode='overwrite')\n",
    "# test_data.write.csv('s3://sagemaker-us-east-1-548775606164/test_data', header=True, mode='overwrite')\n",
    "# train_data.write.format('csv').option('header','true').mode(\"overwrite\").save('s3://sagemaker-us-east-1-548775606164/train_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "94aba723-5127-4935-a335-842946924edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Assuming 'train_data' is your existing DataFrame\n",
    "# Replace these column names with your actual column names\n",
    "columns_to_convert = [\n",
    "    \"Product Price\", \"Quantity\", \"Total Purchase Amount\", \"Returns\", \"Age\",\n",
    "    \"Product Category_Books\", \"Product Category_Clothing\", \"Product Category_Electronics\",\n",
    "    \"Product Category_Home\", \"Gender_Female\", \"Gender_Male\",\n",
    "    \"Payment Method_Cash\", \"Payment Method_Credit Card\", \"Payment Method_PayPal\", \"Churn\"\n",
    "]\n",
    "\n",
    "# Sample data (replace this with your actual DataFrame)\n",
    "# train_data = spark.createDataFrame(train_data, columns_to_convert)\n",
    "\n",
    "# Assuming 'Churn' is your label column\n",
    "label_column = \"Churn\"\n",
    "\n",
    "# Assemble features into a vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=columns_to_convert,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data = assembler.transform(test_data)\n",
    "\n",
    "# Select only 'Churn' (label) and 'features' columns\n",
    "train_data = train_data.select(label_column, \"features\")\n",
    "test_data = test_data.select(label_column, \"features\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "# train_data.show(truncate=False)\n",
    "# test_data.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "67ed11b3-fa29-4f07-8ec4-a3bff3a80fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_data.write.parquet(os.path.join(data_prefix, 'train/train.csv'), mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "a8c4a10f-2192-4f75-bf23-e1850433e7e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data.write.format(\"libsvm\").mode(\"overwrite\").save(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "e802851f-c45f-4ec0-8a87-7ae318aa8949",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_data.write.format(\"libsvm\").mode(\"overwrite\").save(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "67733e39-0ca0-44e4-80c6-8b606a120f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lp-notebooks-datasets/ecom/text-csv'"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "9f376195-67f5-428b-8123-938e25d2cb80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "s3_file = s3fs.S3FileSystem()\n",
    "local_path = \"train\"\n",
    "s3_path = os.path.join(data_bucket,os.path.join(data_prefix, 'train'))\n",
    "s3_file.put(local_path, s3_path, recursive=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "992f1933-344a-4c9c-b40e-f138cf1db790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "s3_file = s3fs.S3FileSystem()\n",
    "local_path = \"test\"\n",
    "s3_path = os.path.join(data_bucket,os.path.join(data_prefix, 'test'))\n",
    "s3_file.put(local_path, s3_path, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "70e70981-b428-4099-8bb4-4e5116fd64d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "# boto3.Session().resource('s3').Bucket(data_bucket).Object(os.path.join(data_prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(data_bucket).Object(os.path.join(data_prefix, 'validation/validation.csv')).upload_file('validation.csv')\n",
    "# boto3.Session().resource('s3').Bucket(data_bucket).Object(os.path.join(data_prefix, 'test/test.csv')).upload_file('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "44bc0e4f-f7a7-4542-9f57-bf6f5a85a93f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training files will be taken from: s3://sagemaker-us-east-1-896303854230/lp-notebooks-datasets/ecom/text-csv/train\n",
      "validtion files will be taken from: s3://sagemaker-us-east-1-896303854230/lp-notebooks-datasets/ecom/text-csv/validation\n",
      "test files will be taken from: s3://sagemaker-us-east-1-896303854230/lp-notebooks-datasets/ecom/text-csv/test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s3_train_data = f\"s3://{data_bucket}/{data_prefix}/train\"\n",
    "print(f\"training files will be taken from: {s3_train_data}\")\n",
    "s3_validation_data = f\"s3://{data_bucket}/{data_prefix}/validation\"\n",
    "print(f\"validtion files will be taken from: {s3_validation_data}\")\n",
    "# print test\n",
    "s3_test_data = f\"s3://{data_bucket}/{data_prefix}/test\"\n",
    "print(f\"test files will be taken from: {s3_test_data}\")\n",
    "# output_location = f\"s3: //{output_bucket}/{output_prefix}/output\"\n",
    "# print(f\"training artifacts output location: {output_location}\")\n",
    "# # generating the session.S3_input () format for fit() accepted by the sak\n",
    "train_data = sagemaker.inputs.TrainingInput (\n",
    "s3_train_data,\n",
    "distribution=\"FullyRepticated\",\n",
    "content_type=\"text/csv\",\n",
    "s3_data_type=\"S3Prefix\",\n",
    "record_wrapping=None,compression=None\n",
    ")\n",
    "\n",
    "s3_validation_data = sagemaker.inputs.TrainingInput (\n",
    "s3_validation_data,\n",
    "distribution=\"FullyRepticated\",\n",
    "content_type=\"text/csv\",\n",
    "s3_data_type=\"S3Prefix\",\n",
    "record_wrapping=None,compression=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e90b7-b19f-4464-aa55-a3128965da5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "e300b47a-b4a7-4130-856c-de1ed30ff260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainingData = (\n",
    "#     spark.read.format(\"libsvm\")\n",
    "#     .option(\"numFeatures\", \"14\")\n",
    "#     .load(\"s3a://sagemaker-us-east-1-548775606164/1p-notebooks-datasets/taxi/text-csv/train/train.csv\".format(region))\n",
    "# )\n",
    "# s3://sagemaker-studio-896303854230-2oke5zb2q4i/sagemaker/DEMO-linear-learner-ecom-regression\n",
    "\n",
    "trainingData = (\n",
    "    spark.read.format(\"libsvm\")\n",
    "    .option(\"header\", \"True\")  # If your CSV has a header row\n",
    "    .option(\"inferSchema\", \"False\")  # Infers the data types of columns\n",
    "    .option(\"numFeatures\", \"15\")\n",
    "    .load(\"s3a://sagemaker-us-east-1-896303854230/lp-notebooks-datasets/ecom/text-csv/train/train\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "d0a31418-8f8a-4671-8b71-a66372521ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testingData = (\n",
    "    spark.read.format(\"libsvm\")\n",
    "    .option(\"header\", \"True\")  # If your CSV has a header row\n",
    "    .option(\"inferSchema\", \"False\")  # Infers the data types of columns\n",
    "    .option(\"numFeatures\", \"15\")\n",
    "    .load(\"s3a://sagemaker-us-east-1-896303854230/lp-notebooks-datasets/ecom/text-csv/test/test\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7385c7-923f-47e9-9ea9-7e5464ae823e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "fd00ac6b-db26-4baa-9ebd-d02e97510f72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for column in columns_to_convert:\n",
    "#     trainingData = trainingData.withColumn(column, col(column).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "61b0bf06-9095-4bcb-b9de-f724ccd89ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assembler = VectorAssembler(inputCols=columns_to_convert, outputCol=\"features\")\n",
    "# trainingData = assembler.transform(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "0565dc35-0f4d-49f1-aab6-f1a456d3fb40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainingData = trainingData.select(\"features\", \"Churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "613f5248-05fa-4d60-8d7f-8b28b855ac67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trainingData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "76bcf6c0-f023-4430-8289-072a15f0f72e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testingData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a75f84fe-b3b0-41d0-85a2-10f6236a7aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #### Done till here\n",
    "# from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "# \tPurchase Date\tProduct Category\tProduct Price\tQuantity\tTotal Purchase Amount\tPayment Method\tCustomer Age\tReturns\tCustomer Name\tAge\tGender\tChurn\n",
    "\n",
    "# # Define your schema based on your data\n",
    "# schema = StructType([\n",
    "#     StructField(\"Customer ID\", IntegerType(), True),\n",
    "#     StructField(\"column2\", StringType(), True),\n",
    "#     # Add more fields as per your data, specifying the column name and data type\n",
    "# ])\n",
    "\n",
    "# # Load data with explicit schema\n",
    "# trainingData = spark.read.format(\"csv\") \\\n",
    "#     .option(\"header\", \"false\") \\\n",
    "#     .schema(schema) \\\n",
    "#     .load(\"s3a://sagemaker-us-east-1-548775606164/1p-notebooks-datasets/taxi/text-csv/train/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "7ddfd524-2fbc-44bd-80f2-3ad01d62ea7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker_pyspark import SageMakerEstimator\n",
    "from sagemaker_pyspark.transformation.serializers import ProtobufRequestRowSerializer,RequestRowSerializer,UnlabeledCSVRequestRowSerializer\n",
    "from sagemaker_pyspark.transformation.deserializers import ProtobufResponseRowDeserializer\n",
    "from sagemaker_pyspark import IAMRole\n",
    "from sagemaker_pyspark import RandomNamePolicyFactory,RandomNamePolicy\n",
    "from sagemaker_pyspark import EndpointCreationPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabaa1bb-93c0-44d6-a1f3-216d510548e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "86771340-1ca8-48f9-9aff-9b3fd30c3c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = sagemaker.image_uris.retrieve(\"linear-learner\", sess.boto_region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "d5632bbb-269c-4a1f-bcc6-214a19db2fb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker_pyspark.transformation.deserializers import LinearLearnerBinaryClassifierProtobufResponseRowDeserializer,ProtobufResponseRowDeserializer,KMeansProtobufResponseRowDeserializer\n",
    "hyperparameters = {\n",
    "    \"predictor_type\": \"binary_classifier\",\n",
    "    \"mini_batch_size\": 100,  # Replace with the actual feature dimension\n",
    "    \"epochs\": 10,\n",
    "}\n",
    "\n",
    "# Create an Estimator\n",
    "# linear_learner_estimator = Estimator(\n",
    "#     image_uri=container,\n",
    "#     role=role,\n",
    "#     instance_count=2,\n",
    "#     instance_type=\"ml.m5.4xlarge\",  # Choose an instance type based on your requirements\n",
    "#     output_path=output_location,  # Specify your S3 bucket for model output\n",
    "#     sagemaker_session=sess,\n",
    "#     hyperparameters=hyperparameters,\n",
    "# )\n",
    "\n",
    "\n",
    "# estimator = SageMakerEstimator(\n",
    "#     trainingImage=container,  # Training image\n",
    "#     modelImage=container,  # Model image\n",
    "#     requestRowSerializer=ProtobufRequestRowSerializer(),\n",
    "#     responseRowDeserializer=ProtobufResponseRowDeserializer(),\n",
    "#     hyperParameters=hyperparameters,  # Set parameters for K-Means\n",
    "#     sagemakerRole=IAMRole(role),\n",
    "#     trainingInstanceType=\"ml.m4.xlarge\",\n",
    "#     trainingInstanceCount=1,\n",
    "#     endpointInstanceType=\"ml.t2.medium\",\n",
    "#     endpointInitialInstanceCount=1,\n",
    "#     trainingSparkDataFormat=\"sagemaker\",\n",
    "#     namePolicyFactory=RandomNamePolicyFactory(\"sparksm-4-\"),\n",
    "#     endpointCreationPolicy=EndpointCreationPolicy.CREATE_ON_TRANSFORM,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c47a98-6c85-4500-a28d-0424fa7669ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "72ffa444-b184-4657-8bb5-61117d8e20ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "08e5e990-014e-4e20-b919-71f92d405814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sagemaker_pyspark.transformation.serializers import CSVSerializer\n",
    "# from sagemaker_pyspark.transformation.deserializers import UnlabeledCSVRequestRowSerializer\n",
    "from sagemaker_pyspark.transformation.serializers import UnlabeledCSVRequestRowSerializer\n",
    "endpoint_config_name = \"my-kmeans-endpoint-config\"\n",
    "kmeans_estimator = KMeansSageMakerEstimator(\n",
    "    sagemakerRole=IAMRole(role),\n",
    "    trainingInstanceType=\"ml.m4.xlarge\",  # Instance type to train K-means on SageMaker\n",
    "    trainingInstanceCount=1,\n",
    "    endpointInstanceType=\"ml.t2.large\",  # Instance type to serve model (endpoint) for inference\n",
    "    endpointInitialInstanceCount=1\n",
    ") \n",
    "# kmeans_estimator._set(endpointConfigName=endpoint_config_name)\n",
    "kmeans_estimator.setFeatureDim(15)\n",
    "kmeans_estimator.setK(2)\n",
    "# kmeans_estimator.setTrainingInput(recordIOData=csv_format_data, contentType=\"text/csv\", serializer=CSVSerializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e467848-155b-4697-9694-0bf17230a312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "ce06006b-17d2-4950-be40-dc6cc13dbcdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/26 23:06:12 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "23/11/26 23:06:12 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "23/11/26 23:06:12 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customModel = kmeans_estimator.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "2b8c6c05-2a3d-4b80-a902-800039cdf199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initialModelEndpointName=customModel.endpointName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "afe9f0e0-5605-49f5-a48d-623e6247f5f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 161:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------------------+---------------+\n",
      "|label|            features|distance_to_cluster|closest_cluster|\n",
      "+-----+--------------------+-------------------+---------------+\n",
      "|  1.0|(15,[0,1,2,3,4,7,...|  1085.364013671875|            0.0|\n",
      "|  0.0|(15,[0,1,2,3,4,7,...|   594.557373046875|            0.0|\n",
      "|  1.0|(15,[0,1,2,3,4,8,...|  1025.041259765625|            0.0|\n",
      "|  0.0|(15,[0,1,2,4,7,9,...| 1197.7076416015625|            0.0|\n",
      "|  0.0|(15,[0,1,2,4,5,9,...| 1002.5492553710938|            1.0|\n",
      "|  1.0|(15,[0,1,2,3,4,8,...| 251.60684204101562|            1.0|\n",
      "|  0.0|(15,[0,1,2,4,5,10...|  705.8165283203125|            1.0|\n",
      "|  1.0|(15,[0,1,2,4,7,9,...| 1368.0238037109375|            1.0|\n",
      "|  0.0|(15,[0,1,2,3,4,8,...|   1039.66357421875|            0.0|\n",
      "|  0.0|(15,[0,1,2,4,8,10...|     695.9775390625|            0.0|\n",
      "|  0.0|(15,[0,1,2,3,4,7,...|  822.4688720703125|            1.0|\n",
      "|  0.0|(15,[0,1,2,4,7,10...|  530.9755249023438|            1.0|\n",
      "|  0.0|(15,[0,1,2,3,4,6,...|  494.2064208984375|            1.0|\n",
      "|  0.0|(15,[0,1,2,4,7,10...|  473.4258117675781|            1.0|\n",
      "|  0.0|(15,[0,1,2,4,8,10...|  675.2708740234375|            0.0|\n",
      "|  1.0|(15,[0,1,2,3,4,6,...|    533.76025390625|            0.0|\n",
      "|  0.0|(15,[0,1,2,3,4,6,...| 397.04974365234375|            0.0|\n",
      "|  0.0|(15,[0,1,2,3,4,8,...| 1084.3748779296875|            0.0|\n",
      "|  0.0|(15,[0,1,2,3,4,5,...|      1232.77734375|            1.0|\n",
      "|  0.0|(15,[0,1,2,4,5,10...| 1089.0753173828125|            1.0|\n",
      "+-----+--------------------+-------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformedData = customModel.transform(testingData)\n",
    "\n",
    "transformedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "dc443344-a1e4-4e73-844b-6fe0e064b4a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint-0d524ba8b028-2023-11-26T23-06-12-155467\n"
     ]
    }
   ],
   "source": [
    "ENDPOINT_NAME = initialModelEndpointName\n",
    "print(ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "36afa90f-6d33-462b-8683-49dfbf63e772",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896303854230-sagemaker-us-east-17ef57664-322a-4342-9975-b04e93881cca/trainingJob-0d524ba8b028-2023-11-26T23-06-12-155467/trainingJob-0d524ba8b028-2023-11-26T23-06-12-155467/output/model.tar.gz\n",
      "arn:aws:iam::896303854230:role/service-role/AmazonSageMaker-ExecutionRole-20231126T103665\n",
      "382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "d8f5a1d2-65c1-469a-90ad-9c0ad222787d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker_pyspark import SageMakerModel\n",
    "from sagemaker_pyspark import EndpointCreationPolicy\n",
    "from sagemaker_pyspark.transformation.serializers import ProtobufRequestRowSerializer\n",
    "from sagemaker_pyspark.transformation.deserializers import KMeansProtobufResponseRowDeserializer\n",
    "\n",
    "attachedModel = SageMakerModel(\n",
    "    existingEndpointName=ENDPOINT_NAME,\n",
    "    endpointCreationPolicy=EndpointCreationPolicy.DO_NOT_CREATE,\n",
    "    endpointInstanceType=None,  # Required\n",
    "    endpointInitialInstanceCount=None,  # Required\n",
    "    requestRowSerializer=ProtobufRequestRowSerializer(\n",
    "        featuresColumnName=\"features\"\n",
    "    ),  # Optional: already default value\n",
    "    responseRowDeserializer=KMeansProtobufResponseRowDeserializer(  # Optional: already default values\n",
    "        distance_to_cluster_column_name=\"distance_to_cluster\",\n",
    "        closest_cluster_column_name=\"closest_cluster\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "fe373e67-3336-4955-96f1-4b361485b740",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 164:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------------------+---------------+\n",
      "|label|            features|distance_to_cluster|closest_cluster|\n",
      "+-----+--------------------+-------------------+---------------+\n",
      "|  1.0|(15,[0,1,2,3,4,7,...|  1085.364013671875|            0.0|\n",
      "|  0.0|(15,[0,1,2,3,4,7,...|   594.557373046875|            0.0|\n",
      "|  1.0|(15,[0,1,2,3,4,8,...|  1025.041259765625|            0.0|\n",
      "|  0.0|(15,[0,1,2,4,7,9,...| 1197.7076416015625|            0.0|\n",
      "|  0.0|(15,[0,1,2,4,5,9,...| 1002.5492553710938|            1.0|\n",
      "|  1.0|(15,[0,1,2,3,4,8,...| 251.60684204101562|            1.0|\n",
      "|  0.0|(15,[0,1,2,4,5,10...|  705.8165283203125|            1.0|\n",
      "|  1.0|(15,[0,1,2,4,7,9,...| 1368.0238037109375|            1.0|\n",
      "|  0.0|(15,[0,1,2,3,4,8,...|   1039.66357421875|            0.0|\n",
      "|  0.0|(15,[0,1,2,4,8,10...|     695.9775390625|            0.0|\n",
      "|  0.0|(15,[0,1,2,3,4,7,...|  822.4688720703125|            1.0|\n",
      "|  0.0|(15,[0,1,2,4,7,10...|  530.9755249023438|            1.0|\n",
      "|  0.0|(15,[0,1,2,3,4,6,...|  494.2064208984375|            1.0|\n",
      "|  0.0|(15,[0,1,2,4,7,10...|  473.4258117675781|            1.0|\n",
      "|  0.0|(15,[0,1,2,4,8,10...|  675.2708740234375|            0.0|\n",
      "|  1.0|(15,[0,1,2,3,4,6,...|    533.76025390625|            0.0|\n",
      "|  0.0|(15,[0,1,2,3,4,6,...| 397.04974365234375|            0.0|\n",
      "|  0.0|(15,[0,1,2,3,4,8,...| 1084.3748779296875|            0.0|\n",
      "|  0.0|(15,[0,1,2,3,4,5,...|      1232.77734375|            1.0|\n",
      "|  0.0|(15,[0,1,2,4,5,10...| 1089.0753173828125|            1.0|\n",
      "+-----+--------------------+-------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformedData2 = attachedModel.transform(testingData)\n",
    "transformedData2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "c36cc5b9-4bc3-462f-94fd-b5f5cf5e8912",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'endpoint-0d524ba8b028-2023-11-26T23-06-12-155467'"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attachedModel.endpointName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "6b1e8507-24cb-4ace-9051-9f2eee72c849",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'endpoint-0d524ba8b028-2023-11-26T23-06-12-155467'"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "60d2ca7b-7e31-4a4f-917d-711bd7ead7a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer, JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "fd27a208-beaa-47bc-ad3f-338178de70de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896303854230-sagemaker-us-east-17ef57664-322a-4342-9975-b04e93881cca/trainingJob-0d524ba8b028-2023-11-26T23-06-12-155467/trainingJob-0d524ba8b028-2023-11-26T23-06-12-155467/output/model.tar.gz\n",
      "arn:aws:iam::896303854230:role/service-role/AmazonSageMaker-ExecutionRole-20231126T103665\n",
      "382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker_pyspark import S3DataPath\n",
    "\n",
    "MODEL_S3_PATH = S3DataPath(customModel.modelPath.bucket, customModel.modelPath.objectPath)\n",
    "MODEL_ROLE_ARN = customModel.modelExecutionRoleARN\n",
    "MODEL_IMAGE_PATH = customModel.modelImage\n",
    "\n",
    "print(MODEL_S3_PATH.bucket + MODEL_S3_PATH.objectPath)\n",
    "print(MODEL_ROLE_ARN)\n",
    "print(MODEL_IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "eca7c6b1-3dff-4c3a-8a49-0530ee561bb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sparkml.model import SparkMLModel,SparkMLPredictor\n",
    "from sagemaker.serializers import LibSVMSerializer\n",
    "sparkml_model = SparkMLPredictor(MODEL_S3_PATH, role=MODEL_ROLE_ARN, spark_version='3.3', sagemaker_session=None,serializer=LibSVMSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "0df04821-11bb-47f8-be86-08d04eb50e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testingData = (\n",
    "    spark.read.format(\"libsvm\")\n",
    "    .option(\"header\", \"False\")  # If your CSV has a header row\n",
    "    .option(\"inferSchema\", \"False\")  # Infers the data types of columns\n",
    "    .option(\"numFeatures\", \"15\")\n",
    "    .load(\"s3a://sagemaker-us-east-1-896303854230/lp-notebooks-datasets/ecom/text-csv/test/test\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671af09-a75e-45b5-b863-1c4e773d5114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01664dac-ed5b-4c5b-b7c3-f3e21c8d8e67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f93da21-1976-409b-b530-cc154d1948ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "118717ef-306a-4cba-9502-1481a544c92a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to handle input format: <class 'pyspark.sql.dataframe.DataFrame'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[559], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msparkml_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestingData\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/base_predictor.py:180\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    134\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     custom_attributes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    140\u001b[0m ):\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the inference from the specified endpoint.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m            as is.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     request_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_request_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_variant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39msagemaker_runtime_client\u001b[38;5;241m.\u001b[39minvoke_endpoint(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_args)\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_response(response)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/base_predictor.py:261\u001b[0m, in \u001b[0;36mPredictor._create_request_args\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m custom_attributes:\n\u001b[1;32m    256\u001b[0m     args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomAttributes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m custom_attributes\n\u001b[1;32m    258\u001b[0m data \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    259\u001b[0m     jumpstart_serialized_data\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, JumpStartSerializablePayload) \u001b[38;5;129;01mand\u001b[39;00m jumpstart_serialized_data\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m )\n\u001b[1;32m    264\u001b[0m args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/base_serializers.py:362\u001b[0m, in \u001b[0;36mLibSVMSerializer.serialize\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to handle input format: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(data))\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to handle input format: <class 'pyspark.sql.dataframe.DataFrame'>"
     ]
    }
   ],
   "source": [
    "sparkml_model.predict(testingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd8342-4f9c-45e8-95ec-fd93e8e09521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ce873152-a512-4da7-ab36-76927e9d9eae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/26 21:08:31 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "23/11/26 21:08:31 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "23/11/26 21:08:31 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2617.fit.\n: java.lang.RuntimeException: Training job couldn't be completed.\n\tat com.amazonaws.services.sagemaker.sparksdk.SageMakerEstimator.runTrainingJob(SageMakerEstimator.scala:415)\n\tat com.amazonaws.services.sagemaker.sparksdk.SageMakerEstimator.fit(SageMakerEstimator.scala:315)\n\tat com.amazonaws.services.sagemaker.sparksdk.SageMakerEstimator.fit(SageMakerEstimator.scala:175)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: com.amazonaws.services.sagemaker.model.AmazonSageMakerException: NUMBER_VALUE can not be converted to a String (Service: AmazonSageMaker; Status Code: 400; Error Code: SerializationException; Request ID: fe8abdbd-f2f7-4fc1-80dd-c1ca770452bc; Proxy: null)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.sagemaker.AmazonSageMakerClient.doInvoke(AmazonSageMakerClient.java:9565)\n\tat com.amazonaws.services.sagemaker.AmazonSageMakerClient.invoke(AmazonSageMakerClient.java:9532)\n\tat com.amazonaws.services.sagemaker.AmazonSageMakerClient.invoke(AmazonSageMakerClient.java:9521)\n\tat com.amazonaws.services.sagemaker.AmazonSageMakerClient.executeCreateTrainingJob(AmazonSageMakerClient.java:2208)\n\tat com.amazonaws.services.sagemaker.AmazonSageMakerClient.createTrainingJob(AmazonSageMakerClient.java:2178)\n\tat com.amazonaws.services.sagemaker.sparksdk.SageMakerEstimator.runTrainingJob(SageMakerEstimator.scala:412)\n\t... 14 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[216], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m train_data_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://sagemaker-us-east-1-548775606164/1p-notebooks-datasets/taxi/text-csv/train\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# trainingData = (\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     spark.read.format(\"csv\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     .option(\"header\", \"True\")  #Trueour CSV has a header row\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# train_data = TrainingInput(\"s3://your-bucket/path/to/training/data\", content_type=\"csv\")\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m customModel \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainingData\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/SageMakerEstimator.py:257\u001b[0m, in \u001b[0;36mSageMakerEstimatorBase.fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fits a SageMakerModel on dataset by running a SageMaker training job.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    JavaSageMakerModel: The Model created by the training job.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_java\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/wrapper.py:78\u001b[0m, in \u001b[0;36mSageMakerJavaWrapper._call_java\u001b[0;34m(self, name, *args)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args:\n\u001b[1;32m     76\u001b[0m     java_args\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_py2j(arg))\n\u001b[0;32m---> 78\u001b[0m java_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSageMakerJavaWrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m SageMakerJavaWrapper\u001b[38;5;241m.\u001b[39m_from_java(java_value)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/wrapper.py:72\u001b[0m, in \u001b[0;36mJavaWrapper._call_java\u001b[0;34m(self, name, *args)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2617.fit.\n: java.lang.RuntimeException: Training job couldn't be completed.\n\tat com.amazonaws.services.sagemaker.sparksdk.SageMakerEstimator.runTrainingJob(SageMakerEstimator.scala:415)\n\tat com.amazonaws.services.sagemaker.sparksdk.SageMakerEstimator.fit(SageMakerEstimator.scala:315)\n\tat com.amazonaws.services.sagemaker.sparksdk.SageMakerEstimator.fit(SageMakerEstimator.scala:175)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: com.amazonaws.services.sagemaker.model.AmazonSageMakerException: NUMBER_VALUE can not be converted to a String (Service: AmazonSageMaker; Status Code: 400; Error Code: SerializationException; Request ID: fe8abdbd-f2f7-4fc1-80dd-c1ca770452bc; Proxy: null)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.sagemaker.AmazonSageMakerClient.doInvoke(AmazonSageMakerClient.java:9565)\n\tat com.amazonaws.services.sagemaker.AmazonSageMakerClient.invoke(AmazonSageMakerClient.java:9532)\n\tat com.amazonaws.services.sagemaker.AmazonSageMakerClient.invoke(AmazonSageMakerClient.java:9521)\n\tat com.amazonaws.services.sagemaker.AmazonSageMakerClient.executeCreateTrainingJob(AmazonSageMakerClient.java:2208)\n\tat com.amazonaws.services.sagemaker.AmazonSageMakerClient.createTrainingJob(AmazonSageMakerClient.java:2178)\n\tat com.amazonaws.services.sagemaker.sparksdk.SageMakerEstimator.runTrainingJob(SageMakerEstimator.scala:412)\n\t... 14 more\n"
     ]
    }
   ],
   "source": [
    "train_data_location = \"s3://sagemaker-us-east-1-548775606164/1p-notebooks-datasets/taxi/text-csv/train\"\n",
    "# trainingData = (\n",
    "#     spark.read.format(\"csv\")\n",
    "#     .option(\"header\", \"True\")  #Trueour CSV has a header row\n",
    "#     .option(\"inferSchema\", \"True\")  # Truers the data types of columns\n",
    "#     .option(\"numFeatures\", \"14\")\n",
    "#     .load(\"s3a://sagemaker-us-east-1-548775606164/1p-notebooks-datasets/taxi/text-csv/train\")\n",
    "# )\n",
    "# cn_regions = [\"cn-north-1\", \"cn-northwest-1\"]\n",
    "# region = boto3.Session().region_name\n",
    "# endpoint_domain = \"com.cn\" if region in cn_regions else \"com\"\n",
    "# spark._jsc.hadoopConfiguration().set(\n",
    "#     \"fs.s3a.endpoint\", \"s3.{}.amazonaws.{}\".format(region, endpoint_domain)\n",
    "# )\n",
    "# train_data = TrainingInput(\"s3://your-bucket/path/to/training/data\", content_type=\"csv\")\n",
    "\n",
    "customModel = estimator.fit(trainingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4795670-f191-431f-928d-a633aa8108b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce31b7-6788-4cfd-9167-664b1cb9acfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb7939-f65a-4f97-94aa-e41fda41a118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f3d4df-4522-4705-b30e-0e1155ecb59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3058ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=d6c49554cf895826e32bacd6cd55016755110847c37859c408d0110bdab56a4c\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aa4659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, max, min\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"YourAppName\").getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# Creating a DataFrame from the sample data\n",
    "df = spark.read.csv(, header=True, inferSchema=True)\n",
    "\n",
    "# 1. General Overview\n",
    "# 1.1 Size of the dataset\n",
    "num_rows = df.count()\n",
    "num_columns = len(df.columns)\n",
    "\n",
    "# 1.2 Missing values\n",
    "missing_values = {col_name: df.filter(col(col_name).isNull()).count() for col_name in df.columns}\n",
    "\n",
    "# 1.3 Data types\n",
    "data_types = {col_name: str(df.schema[col_name].dataType) for col_name in df.columns}\n",
    "\n",
    "# Output for General Overview\n",
    "print(\"1. General Overview:\")\n",
    "print(f\"   1.1 Size of the dataset: {num_rows} rows x {num_columns} columns\")\n",
    "print(\"   1.2 Missing Values:\")\n",
    "for col_name, missing_count in missing_values.items():\n",
    "    print(f\"      {col_name}: {missing_count} missing values\")\n",
    "print(\"   1.3 Data Types:\")\n",
    "for col_name, data_type in data_types.items():\n",
    "    print(f\"      {col_name}: {data_type}\")\n",
    "\n",
    "# 2. Customer Demographics\n",
    "# 2.1 Distribution of customer ages\n",
    "customer_age_distribution = df.groupBy(\"Customer Age\").count().orderBy(\"Customer Age\").collect()\n",
    "\n",
    "# 2.2 Gender distribution\n",
    "gender_distribution = df.groupBy(\"Gender\").count().collect()\n",
    "\n",
    "# 2.3 Check for outliers in customer demographics (assuming age is numeric)\n",
    "age_stats = df.select(\"Customer Age\").summary(\"mean\", \"stddev\", \"min\", \"max\").collect()\n",
    "\n",
    "\n",
    "# 2.3 Check for outliers in customer demographics (assuming age is numeric)\n",
    "try:\n",
    "    age_stats = df.select(\"Customer Age\").summary(\"mean\", \"stddev\", \"min\", \"max\").collect()\n",
    "except ValueError:\n",
    "    # If 'mean' is not supported, use alternative approach\n",
    "    age_stats = df.selectExpr(\"avg(`Customer Age`) as mean\", \"stddev_samp(`Customer Age`) as stddev\", \"min(`Customer Age`) as min\", \"max(`Customer Age`) as max\").collect()\n",
    "\n",
    "\n",
    "\n",
    "# 3. Purchase Behavior\n",
    "# 3.1 Distribution of total purchase amounts\n",
    "purchase_amount_distribution = df.groupBy(\"Total Purchase Amount\").count().orderBy(\"Total Purchase Amount\").collect()\n",
    "\n",
    "# 3.2 Most popular product categories\n",
    "popular_categories = df.groupBy(\"Product Category\").count().orderBy(col(\"count\").desc()).collect()\n",
    "\n",
    "# 3.3 Correlation between product price and quantity\n",
    "correlation = df.stat.corr(\"Product Price\", \"Quantity\")\n",
    "\n",
    "# Output for Purchase Behavior\n",
    "print(\"\\n3. Purchase Behavior:\")\n",
    "print(\"   3.1 Distribution of Total Purchase Amounts:\")\n",
    "# Describe the distribution of total purchase amounts\n",
    "purchase_amount_stats = df.describe(\"Total Purchase Amount\").toPandas()\n",
    "\n",
    "\n",
    "\n",
    "# Plot a histogram of total purchase amounts\n",
    "plt.hist(df.select(\"Total Purchase Amount\").rdd.flatMap(lambda x: x).collect(), bins=20, color='blue', edgecolor='black')\n",
    "plt.title(\"Distribution of Total Purchase Amounts\")\n",
    "plt.xlabel(\"Total Purchase Amount\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "print(\"3.2 The product categories that are popular among customers are:\")\n",
    "\n",
    "# Calculate the count of purchases for each product category\n",
    "product_category_counts = df.groupBy(\"Product Category\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "# Display the most popular product categories\n",
    "print(\"Most Popular Product Categories:\")\n",
    "product_category_counts.show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"   3.3 Correlation between Product Price and Quantity: {correlation}\")\n",
    "\n",
    "# 4. Churn Analysis\n",
    "# 4.1 Overall churn rate\n",
    "churn_rate = df.select(avg(\"Churn\")).collect()[0]['avg(Churn)']\n",
    "\n",
    "# 4.2 Churn distribution across age groups and genders\n",
    "churn_by_age_gender = df.groupBy(\"Age\", \"Gender\").agg(avg(\"Churn\").alias(\"Churn Rate\")).orderBy(\"Age\", \"Gender\").collect()\n",
    "\n",
    "# 4.3 Distribution of churn across different product categories\n",
    "churn_by_category = df.groupBy(\"Product Category\").agg(avg(\"Churn\").alias(\"Churn Rate\")).orderBy(col(\"Churn Rate\").desc()).collect()\n",
    "\n",
    "# Output for Churn Analysis\n",
    "print(\"\\n4. Churn Analysis:\")\n",
    "print(f\"   4.1 Overall Churn Rate: {churn_rate}\")\n",
    "print(\"   4.2 Churn Distribution across Age Groups and Genders:\")\n",
    "for row in churn_by_age_gender:\n",
    "    print(f\"      Age: {row['Age']}, Gender: {row['Gender']}, Churn Rate: {row['Churn Rate']}\")\n",
    "print(\"   4.3 Churn Distribution across Product Categories:\")\n",
    "for row in churn_by_category:\n",
    "    print(f\"      Category: {row['Product Category']}, Churn Rate: {row['Churn Rate']}\")\n",
    "\n",
    "# 5. Payment Methods\n",
    "# 5.1 Preferred payment methods\n",
    "preferred_payment_methods = df.groupBy(\"Payment Method\").count().orderBy(col(\"count\").desc()).collect()\n",
    "\n",
    "# 5.2 Correlation between payment methods and total purchase amounts\n",
    "payment_correlation = df.groupBy(\"Payment Method\").agg(avg(\"Total Purchase Amount\").alias(\"Avg Purchase Amount\")).collect()\n",
    "\n",
    "# Output for Payment Methods\n",
    "print(\"\\n5. Payment Methods:\")\n",
    "print(\"   5.1 Preferred Payment Methods:\")\n",
    "for row in preferred_payment_methods:\n",
    "    print(f\"      Method: {row['Payment Method']}, Count: {row['count']}\")\n",
    "print(\"   5.2 Correlation between Payment Methods and Total Purchase Amounts:\")\n",
    "for row in payment_correlation:\n",
    "    print(f\"      Method: {row['Payment Method']}, Avg Purchase Amount: {row['Avg Purchase Amount']}\")\n",
    "\n",
    "# 6. Returns\n",
    "# 6.1 Frequency and distribution of product returns\n",
    "return_distribution = df.groupBy(\"Returns\").count().collect()\n",
    "\n",
    "# 6.2 Product categories more prone to returns\n",
    "returns_by_category = df.groupBy(\"Product Category\").agg(avg(\"Returns\").alias(\"Return Rate\")).orderBy(col(\"Return Rate\").desc()).collect()\n",
    "\n",
    "# Output for Returns\n",
    "print(\"\\n6. Returns:\")\n",
    "print(\"   6.1 Frequency and Distribution of Product Returns:\")\n",
    "for row in return_distribution:\n",
    "    print(f\"      Returns: {row['Returns']}, Count: {row['count']}\")\n",
    "print(\"   6.2 Product Categories More Prone to Returns:\")\n",
    "for row in returns_by_category:\n",
    "    print(f\"      Category: {row['Product Category']}, Return Rate: {row['Return Rate']}\")\n",
    "\n",
    "# 7. Temporal Analysis (Assuming 'Purchase Date' is a timestamp column)\n",
    "df1 = df.withColumn(\"Purchase Date\", F.to_timestamp(\"Purchase Date\", \"dd-MM-yyyy HH:mm\"))\n",
    "\n",
    "# Extract month and year from the Purchase Date\n",
    "df1 = df1.withColumn(\"Month\", F.month(\"Purchase Date\"))\n",
    "df1 = df1.withColumn(\"Year\", F.year(\"Purchase Date\"))\n",
    "\n",
    "# Aggregate total purchase amounts by month\n",
    "monthly_purchase = df1.groupBy(\"Year\", \"Month\").agg(F.sum(\"Total Purchase Amount\").alias(\"Total Purchase Amount\"))\n",
    "\n",
    "# Convert to Pandas DataFrame for visualization\n",
    "monthly_purchase_pd = monthly_purchase.toPandas()\n",
    "print(\"7.1 Changes in Customer behavious over time\")\n",
    "# Plot total purchase amounts over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=\"Month\", y=\"Total Purchase Amount\", hue=\"Year\", data=monthly_purchase_pd)\n",
    "plt.title(\"Total Purchase Amount Over Time\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Total Purchase Amount\")\n",
    "plt.show()\n",
    "\n",
    "# Identify seasonal trends in product categories (assuming 'Product Category' is a categorical variable)\n",
    "category_trends = df1.groupBy(\"Year\", \"Month\", \"Product Category\").agg(F.sum(\"Total Purchase Amount\").alias(\"Total Purchase Amount\"))\n",
    "\n",
    "# Convert to Pandas DataFrame for visualization\n",
    "category_trends_pd = category_trends.toPandas()\n",
    "print(\"7.2 Seasonal trends in total purchase amounts or product categories\")\n",
    "# Plot total purchase amounts for each product category over time\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.lineplot(x=\"Month\", y=\"Total Purchase Amount\", hue=\"Product Category\", data=category_trends_pd)\n",
    "plt.title(\"Total Purchase Amount by Product Category Over Time\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Total Purchase Amount\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 8. Correlation Analysis\n",
    "# 8.1 Correlations between numerical features\n",
    "numerical_correlations = df.stat.corr(\"Product Price\", \"Quantity\")\n",
    "\n",
    "# 8.2 Correlation with the target variable (Churn)\n",
    "churn_correlations = df.stat.corr(\"Churn\", \"Product Price\"), df.stat.corr(\"Churn\", \"Quantity\")\n",
    "\n",
    "# Output for Correlation Analysis\n",
    "print(\"\\n8. Correlation Analysis:\")\n",
    "print(f\"   8.1 Correlation between Product Price and Quantity: {numerical_correlations}\")\n",
    "print(f\"   8.2 Correlation between Churn and Product Price: {churn_correlations[0]}, Churn and Quantity: {churn_correlations[1]}\")\n",
    "\n",
    "# 9. Customer Insights\n",
    "print(\"\\n9. Customer Insights:\")\n",
    "\n",
    "# Top spending customers\n",
    "top_spending_customers = df.groupBy(\"Customer ID\", \"Customer Name\").agg(sum(\"Total Purchase Amount\").alias(\"TotalSpent\"))\n",
    "top_spending_customers = top_spending_customers.orderBy(\"TotalSpent\", ascending=False)\n",
    "\n",
    "# Customers with the highest frequency of purchases\n",
    "high_frequency_customers = df.groupBy(\"Customer ID\", \"Customer Name\").agg(count(\"Purchase Date\").alias(\"PurchaseFrequency\"))\n",
    "high_frequency_customers = high_frequency_customers.orderBy(\"PurchaseFrequency\", ascending=False)\n",
    "\n",
    "# Show the results\n",
    "print(\"9.1 Top Spending Customers:\")\n",
    "top_spending_customers.show()\n",
    "\n",
    "print(\"9.2 Customers with Highest Frequency of Purchases:\")\n",
    "high_frequency_customers.show()\n",
    "\n",
    "\n",
    "# 10. Product Insights\n",
    "print(\"\\n 10. Product Insights\")\n",
    "# Product categories contribution to total revenue\n",
    "product_revenue = df.groupBy(\"Product Category\").agg(sum(\"Total Purchase Amount\").alias(\"TotalRevenue\"))\n",
    "product_revenue = product_revenue.orderBy(\"TotalRevenue\", ascending=False)\n",
    "\n",
    "# Average price and quantity of products in each category\n",
    "avg_price_quantity = df.groupBy(\"Product Category\").agg(avg(\"Product Price\").alias(\"AvgPrice\"), avg(\"Quantity\").alias(\"AvgQuantity\"))\n",
    "avg_price_quantity = avg_price_quantity.orderBy(\"Product Category\")\n",
    "\n",
    "# Show the results\n",
    "print(\"10.1 Product Categories Contribution to Total Revenue:\")\n",
    "product_revenue.show()\n",
    "\n",
    "print(\"10.2 Average Price and Quantity of Products in Each Category:\")\n",
    "avg_price_quantity.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1377dbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c808dce1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# cett ø3\n",
    "role = sagemaker.get_execution_role()\n",
    "sess= sagemaker.Session()\n",
    "\n",
    "region=boto3.Session().region_name\n",
    "\n",
    "# S3 bucket for training data.\n",
    "# this witt create bucket tike Accountld>'\n",
    "data_bucket=sess.default_bucket()\n",
    "data_prefix = \"lp—notebooks-datasets/taxi/text—csv\"\n",
    "# S3 bucket for saving code and modet artifacts.\n",
    "output_bucket = data_bucket\n",
    "output_prefix =\"sagemaker/DEMO-linear-learner-taxifare-regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6f16e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_train=\"testingset.csv\"\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(file_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eba50efb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://security.debian.org/debian-security bullseye-security InRelease\n",
      "Hit:2 http://deb.debian.org/debian bullseye InRelease\n",
      "Hit:3 http://deb.debian.org/debian bullseye-updates InRelease\n",
      "Reading package lists... Done\u001b[33m\u001b[33m\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "31 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "software-properties-common is already the newest version (0.96.20.2-2.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "default-jdk is already the newest version (2:1.11-72).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
      "openjdk 11.0.21 2023-10-17\n",
      "OpenJDK Runtime Environment (build 11.0.21+9-post-Debian-1deb11u1)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.21+9-post-Debian-1deb11u1, mixed mode, sharing)\n",
      "bin  conf  docs  include  jmods  legal\tlib  man  release\n",
      "env: JAVA_HOME='/usr/lib/jvm/java-11-openjdk-amd64'\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/geerlingguy/ansible-role-java/issues/64\n",
    "!mkdir -p /usr/share/man/man1\n",
    "\n",
    "# https://stackoverflow.com/a/61902164/4281353\n",
    "!apt update -y\n",
    "!apt install software-properties-common -y\n",
    "!apt install default-jdk -y\n",
    "\n",
    "!java --version\n",
    "!ls '/usr/lib/jvm/java-11-openjdk-amd64'\n",
    "%env JAVA_HOME='/usr/lib/jvm/java-11-openjdk-amd64'\n",
    "\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a45ade7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94301d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c455e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52fd3d80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker_pyspark.algorithms import KMeansSageMakerEstimator\n",
    "from sagemaker_pyspark import SageMakerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75074ce2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker_pyspark in /opt/conda/lib/python3.10/site-packages (1.4.5)\n",
      "Collecting pyspark==3.3.0 (from sagemaker_pyspark)\n",
      "  Using cached pyspark-3.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sagemaker_pyspark) (1.26.0)\n",
      "Collecting py4j==0.10.9.5 (from pyspark==3.3.0->sagemaker_pyspark)\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.7\n",
      "    Uninstalling py4j-0.10.9.7:\n",
      "      Successfully uninstalled py4j-0.10.9.7\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.5.0\n",
      "    Uninstalling pyspark-3.5.0:\n",
      "      Successfully uninstalled pyspark-3.5.0\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sagemaker_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a057c8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.10/site-packages (2.4.8)\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.0-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.7\n",
      "    Uninstalling py4j-0.10.7:\n",
      "      Successfully uninstalled py4j-0.10.7\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 2.4.8\n",
      "    Uninstalling pyspark-2.4.8:\n",
      "      Successfully uninstalled pyspark-2.4.8\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-pyspark 1.4.5 requires pyspark==3.3.0, but you have pyspark 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed py4j-0.10.9.7 pyspark-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3f4e0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----------------+-------------+--------+---------------------+--------------+------------+-------+--------------+---+------+-----+\n",
      "|Customer ID|      Purchase Date|Product Category|Product Price|Quantity|Total Purchase Amount|Payment Method|Customer Age|Returns| Customer Name|Age|Gender|Churn|\n",
      "+-----------+-------------------+----------------+-------------+--------+---------------------+--------------+------------+-------+--------------+---+------+-----+\n",
      "|      44605|2023-05-03 21:30:02|            Home|          177|       1|                 2427|        PayPal|          31|    1.0|   John Rivera| 31|Female|    0|\n",
      "|      44605|2021-05-16 13:57:44|     Electronics|          174|       3|                 2448|        PayPal|          31|    1.0|   John Rivera| 31|Female|    0|\n",
      "|      44605|2020-07-13 06:16:57|           Books|          413|       1|                 2345|   Credit Card|          31|    1.0|   John Rivera| 31|Female|    0|\n",
      "|      44605|2023-01-17 13:14:36|     Electronics|          396|       3|                  937|          Cash|          31|    0.0|   John Rivera| 31|Female|    0|\n",
      "|      44605|2021-05-01 11:29:27|           Books|          259|       4|                 2598|        PayPal|          31|    1.0|   John Rivera| 31|Female|    0|\n",
      "|      13738|2022-08-25 06:48:33|            Home|          191|       3|                 3722|   Credit Card|          27|    1.0|Lauren Johnson| 27|Female|    0|\n",
      "|      13738|2023-07-25 05:17:24|     Electronics|          205|       1|                 2773|   Credit Card|          27|   NULL|Lauren Johnson| 27|Female|    0|\n",
      "|      13738|2023-02-05 19:31:48|           Books|          370|       5|                 1486|          Cash|          27|    1.0|Lauren Johnson| 27|Female|    0|\n",
      "|      13738|2021-12-21 03:29:05|            Home|           12|       2|                 2175|          Cash|          27|   NULL|Lauren Johnson| 27|Female|    0|\n",
      "|      13738|2023-02-09 00:53:14|     Electronics|           40|       4|                 4327|          Cash|          27|    0.0|Lauren Johnson| 27|Female|    0|\n",
      "|      33969|2023-02-28 19:58:23|        Clothing|          410|       3|                 5018|   Credit Card|          27|   NULL|   Carol Allen| 27|  Male|    0|\n",
      "|      33969|2023-01-05 11:15:27|            Home|          304|       1|                 3883|        PayPal|          27|    1.0|   Carol Allen| 27|  Male|    0|\n",
      "|      33969|2023-07-18 23:36:50|           Books|           54|       2|                 4187|        PayPal|          27|    0.0|   Carol Allen| 27|  Male|    0|\n",
      "|      33969|2021-12-20 23:44:57|     Electronics|          428|       4|                 2289|          Cash|          27|    0.0|   Carol Allen| 27|  Male|    0|\n",
      "|      33969|2020-03-07 21:31:35|           Books|          281|       1|                 3810|          Cash|          27|    0.0|   Carol Allen| 27|  Male|    0|\n",
      "|      33969|2022-07-21 04:25:44|            Home|          193|       2|                 3198|   Credit Card|          27|    0.0|   Carol Allen| 27|  Male|    0|\n",
      "|      33969|2023-07-05 15:01:04|        Clothing|          473|       3|                 2881|   Credit Card|          27|    1.0|   Carol Allen| 27|  Male|    0|\n",
      "|      42650|2020-10-18 23:38:52|           Books|          127|       5|                 3347|          Cash|          20|    0.0|  Curtis Smith| 20|Female|    0|\n",
      "|      42650|2020-05-17 17:02:36|            Home|          284|       2|                 3531|   Credit Card|          20|    1.0|  Curtis Smith| 20|Female|    0|\n",
      "|      42650|2022-03-18 13:52:08|     Electronics|          256|       2|                 3548|   Credit Card|          20|    0.0|  Curtis Smith| 20|Female|    0|\n",
      "+-----------+-------------------+----------------+-------------+--------+---------------------+--------------+------------+-------+--------------+---+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sagemaker_pyspark\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.2 pyspark-shell'\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "role = get_execution_role()\n",
    "\n",
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "jars = sagemaker_pyspark.classpath_jars()\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "# spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"example\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.awsAccessKeyId\", \"your-access-key-id\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.awsSecretAccessKey\", \"your-secret-access-key\") \\\n",
    "#     .getOrCreate()\n",
    "# spark = (\n",
    "#     SparkSession.builder.config(\"spark.driver.extraClassPath\", classpath)\n",
    "#     .master(\"local[*]\")\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "#main\n",
    "\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import sagemaker_pyspark\n",
    "import botocore.session\n",
    "\n",
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars())))\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf) \\\n",
    "    .config('fs.s3a.access.key','AKIAX7RMTG6KERGGBYZW' )\n",
    "    .config('fs.s3a.secret.key','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')\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "    .appName(\"schema_test\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "df = spark.read.csv(\"ecommerce_customer_data_large.csv\", header=True, inferSchema=True, encoding='utf-8')\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34d08ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIAX7RMTG6KERGGBYZW\")\n",
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"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\")\n",
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "# spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider\")\n",
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"eu-west-3.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340bfc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e983da41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+--------+---------------------+--------------+-------+---+------+-----+\n",
      "|Product Category|Product Price|Quantity|Total Purchase Amount|Payment Method|Returns|Age|Gender|Churn|\n",
      "+----------------+-------------+--------+---------------------+--------------+-------+---+------+-----+\n",
      "|            Home|          177|       1|                 2427|        PayPal|    1.0| 31|Female|    0|\n",
      "|     Electronics|          174|       3|                 2448|        PayPal|    1.0| 31|Female|    0|\n",
      "|           Books|          413|       1|                 2345|   Credit Card|    1.0| 31|Female|    0|\n",
      "|     Electronics|          396|       3|                  937|          Cash|    0.0| 31|Female|    0|\n",
      "|           Books|          259|       4|                 2598|        PayPal|    1.0| 31|Female|    0|\n",
      "|            Home|          191|       3|                 3722|   Credit Card|    1.0| 27|Female|    0|\n",
      "|     Electronics|          205|       1|                 2773|   Credit Card|   NULL| 27|Female|    0|\n",
      "|           Books|          370|       5|                 1486|          Cash|    1.0| 27|Female|    0|\n",
      "|            Home|           12|       2|                 2175|          Cash|   NULL| 27|Female|    0|\n",
      "|     Electronics|           40|       4|                 4327|          Cash|    0.0| 27|Female|    0|\n",
      "|        Clothing|          410|       3|                 5018|   Credit Card|   NULL| 27|  Male|    0|\n",
      "|            Home|          304|       1|                 3883|        PayPal|    1.0| 27|  Male|    0|\n",
      "|           Books|           54|       2|                 4187|        PayPal|    0.0| 27|  Male|    0|\n",
      "|     Electronics|          428|       4|                 2289|          Cash|    0.0| 27|  Male|    0|\n",
      "|           Books|          281|       1|                 3810|          Cash|    0.0| 27|  Male|    0|\n",
      "|            Home|          193|       2|                 3198|   Credit Card|    0.0| 27|  Male|    0|\n",
      "|        Clothing|          473|       3|                 2881|   Credit Card|    1.0| 27|  Male|    0|\n",
      "|           Books|          127|       5|                 3347|          Cash|    0.0| 20|Female|    0|\n",
      "|            Home|          284|       2|                 3531|   Credit Card|    1.0| 20|Female|    0|\n",
      "|     Electronics|          256|       2|                 3548|   Credit Card|    0.0| 20|Female|    0|\n",
      "+----------------+-------------+--------+---------------------+--------------+-------+---+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=df.drop(*['Customer ID','Customer Name','Customer Age','Purchase Date'])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78653e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-------------+--------+---------------------+--------------+-------+------+------+------+\n",
      "|summary|Product Category|Product Price|Quantity|Total Purchase Amount|Payment Method|Returns|   Age|Gender| Churn|\n",
      "+-------+----------------+-------------+--------+---------------------+--------------+-------+------+------+------+\n",
      "|  count|          250000|       250000|  250000|               250000|        250000| 202618|250000|250000|250000|\n",
      "+-------+----------------+-------------+--------+---------------------+--------------+-------+------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summary_df=df1.describe()\n",
    "summary_df.filter(summary_df[\"summary\"]==\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4111181",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-------------+--------+---------------------+--------------+-------+------+------+------+\n",
      "|summary|Product Category|Product Price|Quantity|Total Purchase Amount|Payment Method|Returns|   Age|Gender| Churn|\n",
      "+-------+----------------+-------------+--------+---------------------+--------------+-------+------+------+------+\n",
      "|  count|          250000|       250000|  250000|               250000|        250000| 202618|250000|250000|250000|\n",
      "+-------+----------------+-------------+--------+---------------------+--------------+-------+------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "null_counts = df1.select([col(c).alias(c) for c in df1.columns]).summary(\"count\")\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a30e1de8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = df1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b62a97a-cc43-4ab7-a1e2-b34012d50f34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "994596e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------------------+-------+---+----------------------+-------------------------+----------------------------+---------------------+-------------+-----------+-------------------+--------------------------+---------------------+-----+\n",
      "|Product Price|Quantity|Total Purchase Amount|Returns|Age|Product Category_Books|Product Category_Clothing|Product Category_Electronics|Product Category_Home|Gender_Female|Gender_Male|Payment Method_Cash|Payment Method_Credit Card|Payment Method_PayPal|Churn|\n",
      "+-------------+--------+---------------------+-------+---+----------------------+-------------------------+----------------------------+---------------------+-------------+-----------+-------------------+--------------------------+---------------------+-----+\n",
      "|177          |1       |2427                 |1.0    |31 |0                     |0                        |0                           |1                    |1            |0          |0                  |0                         |1                    |0    |\n",
      "|174          |3       |2448                 |1.0    |31 |0                     |0                        |1                           |0                    |1            |0          |0                  |0                         |1                    |0    |\n",
      "|413          |1       |2345                 |1.0    |31 |1                     |0                        |0                           |0                    |1            |0          |0                  |1                         |0                    |0    |\n",
      "|396          |3       |937                  |0.0    |31 |0                     |0                        |1                           |0                    |1            |0          |1                  |0                         |0                    |0    |\n",
      "|259          |4       |2598                 |1.0    |31 |1                     |0                        |0                           |0                    |1            |0          |0                  |0                         |1                    |0    |\n",
      "|191          |3       |3722                 |1.0    |27 |0                     |0                        |0                           |1                    |1            |0          |0                  |1                         |0                    |0    |\n",
      "|205          |1       |2773                 |0.0    |27 |0                     |0                        |1                           |0                    |1            |0          |0                  |1                         |0                    |0    |\n",
      "|370          |5       |1486                 |1.0    |27 |1                     |0                        |0                           |0                    |1            |0          |1                  |0                         |0                    |0    |\n",
      "|12           |2       |2175                 |0.0    |27 |0                     |0                        |0                           |1                    |1            |0          |1                  |0                         |0                    |0    |\n",
      "|40           |4       |4327                 |0.0    |27 |0                     |0                        |1                           |0                    |1            |0          |1                  |0                         |0                    |0    |\n",
      "|410          |3       |5018                 |0.0    |27 |0                     |1                        |0                           |0                    |0            |1          |0                  |1                         |0                    |0    |\n",
      "|304          |1       |3883                 |1.0    |27 |0                     |0                        |0                           |1                    |0            |1          |0                  |0                         |1                    |0    |\n",
      "|54           |2       |4187                 |0.0    |27 |1                     |0                        |0                           |0                    |0            |1          |0                  |0                         |1                    |0    |\n",
      "|428          |4       |2289                 |0.0    |27 |0                     |0                        |1                           |0                    |0            |1          |1                  |0                         |0                    |0    |\n",
      "|281          |1       |3810                 |0.0    |27 |1                     |0                        |0                           |0                    |0            |1          |1                  |0                         |0                    |0    |\n",
      "|193          |2       |3198                 |0.0    |27 |0                     |0                        |0                           |1                    |0            |1          |0                  |1                         |0                    |0    |\n",
      "|473          |3       |2881                 |1.0    |27 |0                     |1                        |0                           |0                    |0            |1          |0                  |1                         |0                    |0    |\n",
      "|127          |5       |3347                 |0.0    |20 |1                     |0                        |0                           |0                    |1            |0          |1                  |0                         |0                    |0    |\n",
      "|284          |2       |3531                 |1.0    |20 |0                     |0                        |0                           |1                    |1            |0          |0                  |1                         |0                    |0    |\n",
      "|256          |2       |3548                 |0.0    |20 |0                     |0                        |1                           |0                    |1            |0          |0                  |1                         |0                    |0    |\n",
      "+-------------+--------+---------------------+-------+---+----------------------+-------------------------+----------------------------+---------------------+-------------+-----------+-------------------+--------------------------+---------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import IntegerType\n",
    "import numpy as np\n",
    "\n",
    "prod_categories = df1.select('Product Category').distinct().rdd.flatMap(lambda x : x).collect()\n",
    "gender_categories = df1.select('Gender').distinct().rdd.flatMap(lambda x : x).collect()\n",
    "payment_categories = df1.select('Payment Method').distinct().rdd.flatMap(lambda x : x).collect()\n",
    "prod_categories.sort()\n",
    "gender_categories.sort()\n",
    "payment_categories.sort()\n",
    "for category in prod_categories:\n",
    "    function = udf(lambda item: 1 if item == category else 0, IntegerType())\n",
    "    new_column_name = 'Product Category'+'_'+category\n",
    "    df1 = df1.withColumn(new_column_name, function(col('Product Category')))\n",
    "\n",
    "for category in gender_categories:\n",
    "    function = udf(lambda item: 1 if item == category else 0, IntegerType())\n",
    "    new_column_name = 'Gender'+'_'+category\n",
    "    df1 = df1.withColumn(new_column_name, function(col('Gender')))\n",
    "\n",
    "for category in payment_categories:\n",
    "    function = udf(lambda item: 1 if item == category else 0, IntegerType())\n",
    "    new_column_name = 'Payment Method'+'_'+category\n",
    "    df1 = df1.withColumn(new_column_name, function(col('Payment Method')))\n",
    "df1=df1.drop(*['Product Category','Payment Method','Gender'])\n",
    "df1_cols=df1.drop(\"Churn\").columns\n",
    "df1_cols=np.array(df1_cols)\n",
    "df1_cols = np.append(df1_cols, \"Churn\")\n",
    "df1_cols=list(df1_cols)\n",
    "df1 = df1.selectExpr(*[f\"`{col}`\" for col in df1_cols])\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37e33d15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d33a8b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "# 53 bucket for training data.\n",
    "# this will create bucket like 'Sagemaker-eregions-eYour Accountids！\n",
    "data_bucket=sess.default_bucket()\n",
    "data_prefix = \"1p-notebooks-datasets/taxi/text-csv\"\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "output_bucket =data_bucket\n",
    "output_prefix = \"sagemaker/DEMO-linear-learner-ecom-regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f23d05f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-548775606164/sagemaker/DEMO-linear-learner-ecom-regression/output'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_location=f\"s3://{output_bucket}/{output_prefix}/output\"\n",
    "output_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccf42f83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import boto3\n",
    "\n",
    "# # Create an S3 client\n",
    "# s3 = boto3.client('s3')\n",
    "\n",
    "# # List all S3 buckets\n",
    "# response = s3.list_buckets()\n",
    "\n",
    "# # Print bucket names\n",
    "# for bucket in response['Buckets']:\n",
    "#     print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e610fc49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09139dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pyspark\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.2 pyspark-shell'\n",
    "# from pyspark.sql import SQLContext\n",
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext()\n",
    "# sqlContext = SQLContext(sc)\n",
    "# # filePath = \"s3a://yourBucket/yourFile.parquet\"\n",
    "# # df = sqlContext.read.parquet(filePath) # Parquet file read example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35c9a174",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "columns_to_convert = [\n",
    "    \"Product Price\", \"Quantity\", \"Total Purchase Amount\", \"Returns\", \"Age\",\n",
    "    \"Product Category_Books\", \"Product Category_Clothing\", \"Product Category_Electronics\",\n",
    "    \"Product Category_Home\", \"Gender_Female\", \"Gender_Male\",\n",
    "    \"Payment Method_Cash\", \"Payment Method_Credit Card\", \"Payment Method_PayPal\", \"Churn\"\n",
    "]\n",
    "shuffled_df = df1.sample(fraction=1.0, seed=1729)\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    shuffled_df = shuffled_df.withColumn(column, col(column).cast(\"int\"))\n",
    "# Use randomSplit with weights representing the fraction of each split\n",
    "train_data, validation_data, test_data = shuffled_df.randomSplit([0.7, 0.2, 0.1], seed=1729)\n",
    "train_data = train_data.toPandas()\n",
    "validation_data = validation_data.toPandas()\n",
    "test_data = test_data.toPandas()\n",
    "train_data.to_csv('train.csv',header=True,index=False)\n",
    "validation_data.to_csv('validation.csv',header=True,index=False)\n",
    "test_data.to_csv('test.csv',header=False,index=False)\n",
    "# train_data.write.csv('s3://sagemaker-us-east-1-548775606164/train_data', header=True, mode='overwrite')\n",
    "# validation_data.write.csv('s3://sagemaker-us-east-1-548775606164/validation_data', header=True, mode='overwrite')\n",
    "# test_data.write.csv('s3://sagemaker-us-east-1-548775606164/test_data', header=True, mode='overwrite')\n",
    "# train_data.write.format('csv').option('header','true').mode(\"overwrite\").save('s3://sagemaker-us-east-1-548775606164/train_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b565f8f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_data.count(),validation_data.count(),validation_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "104862d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "boto3.Session().resource('s3').Bucket(data_bucket).Object(os.path.join(data_prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(data_bucket).Object(os.path.join(data_prefix, 'validation/validation.csv')).upload_file('validation.csv')\n",
    "boto3.Session().resource('s3').Bucket(data_bucket).Object(os.path.join(data_prefix, 'test/test.csv')).upload_file('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "18b25283",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training files will be taken from: s3://sagemaker-us-east-1-548775606164/1p-notebooks-datasets/taxi/text-csv/train\n",
      "validtion files will be taken from: s3://sagemaker-us-east-1-548775606164/1p-notebooks-datasets/taxi/text-csv/validation\n",
      "test files will be taken from: s3://sagemaker-us-east-1-548775606164/1p-notebooks-datasets/taxi/text-csv/test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s3_train_data = f\"s3://{data_bucket}/{data_prefix}/train\"\n",
    "print(f\"training files will be taken from: {s3_train_data}\")\n",
    "s3_validation_data = f\"s3://{data_bucket}/{data_prefix}/validation\"\n",
    "print(f\"validtion files will be taken from: {s3_validation_data}\")\n",
    "# print test\n",
    "s3_test_data = f\"s3://{data_bucket}/{data_prefix}/test\"\n",
    "print(f\"test files will be taken from: {s3_test_data}\")\n",
    "# output_location = f\"s3: //{output_bucket}/{output_prefix}/output\"\n",
    "# print(f\"training artifacts output location: {output_location}\")\n",
    "# # generating the session.S3_input () format for fit() accepted by the sak\n",
    "train_data = sagemaker.inputs.TrainingInput (\n",
    "s3_train_data,\n",
    "distribution=\"FullyRepticated\",\n",
    "content_type=\"text/csv\",\n",
    "s3_data_type=\"S3Prefix\",\n",
    "record_wrapping=None,compression=None\n",
    ")\n",
    "\n",
    "s3_validation_data = sagemaker.inputs.TrainingInput (\n",
    "s3_validation_data,\n",
    "distribution=\"FullyRepticated\",\n",
    "content_type=\"text/csv\",\n",
    "s3_data_type=\"S3Prefix\",\n",
    "record_wrapping=None,compression=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86c24882",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/26 02:34:49 WARN FileSystem: Failed to initialize fileystem s3a://sagemaker-us-east-1-548775606164/lp-notebooks-datasets/taxi/text-csv/train/train.csv: java.lang.NumberFormatException: For input string: \"64M\"\n",
      "23/11/26 02:34:49 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://sagemaker-us-east-1-548775606164/lp-notebooks-datasets/taxi/text-csv/train/train.csv.\n",
      "java.lang.NumberFormatException: For input string: \"64M\"\n",
      "\tat java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
      "\tat java.base/java.lang.Long.parseLong(Long.java:692)\n",
      "\tat java.base/java.lang.Long.parseLong(Long.java:817)\n",
      "\tat org.apache.hadoop.conf.Configuration.getLong(Configuration.java:1586)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:248)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/11/26 02:34:49 WARN FileSystem: Failed to initialize fileystem s3a://sagemaker-us-east-1-548775606164/lp-notebooks-datasets/taxi/text-csv/train/train.csv: java.lang.NumberFormatException: For input string: \"64M\"\n"
     ]
    },
    {
     "ename": "NumberFormatException",
     "evalue": "For input string: \"64M\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNumberFormatException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# trainingData = (\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     spark.read.format(\"libsvm\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     .option(\"numFeatures\", \"14\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     .load(\"s3a://sagemaker-us-east-1-548775606164/1p-notebooks-datasets/taxi/text-csv/train/train.csv\".format(region))\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m      8\u001b[0m trainingData \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      9\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfalse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# If your CSV has a header row\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minferSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Infers the data types of columns\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumFeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m14\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3a://sagemaker-us-east-1-548775606164/lp-notebooks-datasets/taxi/text-csv/train/train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mNumberFormatException\u001b[0m: For input string: \"64M\""
     ]
    }
   ],
   "source": [
    "# trainingData = (\n",
    "#     spark.read.format(\"libsvm\")\n",
    "#     .option(\"numFeatures\", \"14\")\n",
    "#     .load(\"s3a://sagemaker-us-east-1-548775606164/1p-notebooks-datasets/taxi/text-csv/train/train.csv\".format(region))\n",
    "# )\n",
    "\n",
    "\n",
    "trainingData = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"false\")  # If your CSV has a header row\n",
    "    .option(\"inferSchema\", \"true\")  # Infers the data types of columns\n",
    "    .option(\"numFeatures\", \"14\")\n",
    "    .load(\"s3a://sagemaker-us-east-1-548775606164/lp-notebooks-datasets/taxi/text-csv/train/train.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ce7db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aed0afcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Done till here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "560dd050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = sagemaker.image_uris.retrieve(\"linear-learner\", sess.boto_region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52f22629",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 21\u001b[0m\n\u001b[1;32m      3\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictor_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmini_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m,  \u001b[38;5;66;03m# Replace with the actual feature dimension\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create an Estimator\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# linear_learner_estimator = Estimator(\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     image_uri=container,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#     hyperparameters=hyperparameters,\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[43mSageMakerEstimator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainingImage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Training image\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodelImage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Model image\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequestRowSerializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mProtobufRequestRowSerializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponseRowDeserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLinearLearnerBinaryClassifierProtobufResponseRowDeserializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperParameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set parameters for K-Means\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43msagemakerRole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIAMRole\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainingInstanceType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mml.m4.xlarge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainingInstanceCount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpointInstanceType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mml.t2.medium\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpointInitialInstanceCount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainingSparkDataFormat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msagemaker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnamePolicyFactory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRandomNamePolicyFactory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msparksm-4-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpointCreationPolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEndpointCreationPolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCREATE_ON_TRANSFORM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/SageMakerEstimator.py:439\u001b[0m, in \u001b[0;36mSageMakerEstimator.__init__\u001b[0;34m(self, trainingImage, modelImage, trainingInstanceType, trainingInstanceCount, endpointInstanceType, endpointInitialInstanceCount, requestRowSerializer, responseRowDeserializer, hyperParameters, trainingInputS3DataPath, trainingOutputS3DataPath, trainingInstanceVolumeSizeInGB, trainingProjectedColumns, trainingChannelName, trainingContentType, trainingS3DataDistribution, trainingSparkDataFormat, trainingSparkDataFormatOptions, trainingInputMode, trainingCompressionCodec, trainingMaxRuntimeInSeconds, trainingKmsKeyId, modelEnvironmentVariables, endpointCreationPolicy, sagemakerClient, sagemakerRole, s3Client, stsClient, modelPrependInputRowsToTransformationRows, deleteStagingDataAfterTraining, namePolicyFactory, uid)\u001b[0m\n\u001b[1;32m    436\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlocals\u001b[39m()\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSageMakerEstimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/SageMakerEstimator.py:112\u001b[0m, in \u001b[0;36mSageMakerEstimatorBase.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;129m@keyword_only\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28msuper\u001b[39m(SageMakerEstimatorBase, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resetUid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_java(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muid\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/SageMakerEstimator.py:443\u001b[0m, in \u001b[0;36mSageMakerEstimator._get_java_obj\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_java_obj\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mSageMakerEstimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainingImage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodelImage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msagemakerRole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainingInstanceType\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainingInstanceCount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mendpointInstanceType\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mendpointInitialInstanceCount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrequestRowSerializer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresponseRowDeserializer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainingInputS3DataPath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainingOutputS3DataPath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainingInstanceVolumeSizeInGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mOption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainingProjectedColumns\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainingChannelName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mOption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainingContentType\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainingS3DataDistribution\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainingSparkDataFormat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainingSparkDataFormatOptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainingInputMode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mOption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainingCompressionCodec\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainingMaxRuntimeInSeconds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mOption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainingKmsKeyId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodelEnvironmentVariables\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mendpointCreationPolicy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msagemakerClient\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms3Client\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstsClient\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodelPrependInputRowsToTransformationRows\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdeleteStagingDataAfterTraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnamePolicyFactory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhyperParameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/wrapper.py:59\u001b[0m, in \u001b[0;36mSageMakerJavaWrapper._new_java_obj\u001b[0;34m(self, java_class, *args)\u001b[0m\n\u001b[1;32m     57\u001b[0m java_args \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args:\n\u001b[0;32m---> 59\u001b[0m     java_args\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_py2j\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m JavaWrapper\u001b[38;5;241m.\u001b[39m_new_java_obj(java_class, \u001b[38;5;241m*\u001b[39mjava_args)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/wrapper.py:43\u001b[0m, in \u001b[0;36mSageMakerJavaWrapper._py2j\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaWrapper\u001b[38;5;241m.\u001b[39m_new_java_obj(\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.types.DataType.fromJson\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(arg\u001b[38;5;241m.\u001b[39mjsonValue())\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, SageMakerJavaWrapper):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/IAMRoleResource.py:54\u001b[0m, in \u001b[0;36mIAMRole._to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_java\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[43mIAMRole\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrole\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/wrapper.py:61\u001b[0m, in \u001b[0;36mSageMakerJavaWrapper._new_java_obj\u001b[0;34m(self, java_class, *args)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args:\n\u001b[1;32m     59\u001b[0m     java_args\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_py2j(arg))\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mJavaWrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjava_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/wrapper.py:86\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjava_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker_pyspark.transformation.deserializers import LinearLearnerBinaryClassifierProtobufResponseRowDeserializer\n",
    "hyperparameters = {\n",
    "    \"predictor_type\": \"binary_classifier\",\n",
    "    \"mini_batch_size\": 100,  # Replace with the actual feature dimension\n",
    "    \"epochs\": 10,\n",
    "}\n",
    "\n",
    "# Create an Estimator\n",
    "# linear_learner_estimator = Estimator(\n",
    "#     image_uri=container,\n",
    "#     role=role,\n",
    "#     instance_count=2,\n",
    "#     instance_type=\"ml.m5.4xlarge\",  # Choose an instance type based on your requirements\n",
    "#     output_path=output_location,  # Specify your S3 bucket for model output\n",
    "#     sagemaker_session=sess,\n",
    "#     hyperparameters=hyperparameters,\n",
    "# )\n",
    "\n",
    "\n",
    "estimator = SageMakerEstimator(\n",
    "    trainingImage=container,  # Training image\n",
    "    modelImage=container,  # Model image\n",
    "    requestRowSerializer=ProtobufRequestRowSerializer(),\n",
    "    responseRowDeserializer=LinearLearnerBinaryClassifierProtobufResponseRowDeserializer(),\n",
    "    hyperParameters=hyperparameters,  # Set parameters for K-Means\n",
    "    sagemakerRole=IAMRole(role),\n",
    "    trainingInstanceType=\"ml.m4.xlarge\",\n",
    "    trainingInstanceCount=1,\n",
    "    endpointInstanceType=\"ml.t2.medium\",\n",
    "    endpointInitialInstanceCount=1,\n",
    "    trainingSparkDataFormat=\"sagemaker\",\n",
    "    namePolicyFactory=RandomNamePolicyFactory(\"sparksm-4-\"),\n",
    "    endpointCreationPolicy=EndpointCreationPolicy.CREATE_ON_TRANSFORM,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ed285b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o430.fit.\n: java.lang.ClassNotFoundException: org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat java.base/java.lang.Class.forName0(Native Method)\n\tat java.base/java.lang.Class.forName(Class.java:398)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.internal.io.FileCommitProtocol$.instantiate(FileCommitProtocol.scala:213)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat com.amazonaws.services.sagemaker.sparksdk.internal.DataUploader.writeData(DataUploader.scala:111)\n\tat com.amazonaws.services.sagemaker.sparksdk.internal.DataUploader.uploadData(DataUploader.scala:90)\n\tat com.amazonaws.services.sagemaker.sparksdk.SageMakerEstimator.fit(SageMakerEstimator.scala:302)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 17\u001b[0m\n\u001b[1;32m     11\u001b[0m endpoint_domain \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom.cn\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m region \u001b[38;5;129;01min\u001b[39;00m cn_regions \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# spark._jsc.hadoopConfiguration().set(\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     \"fs.s3a.endpoint\", \"s3.{}.amazonaws.{}\".format(region, endpoint_domain)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# train_data = TrainingInput(\"s3://your-bucket/path/to/training/data\", content_type=\"csv\")\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m customModel \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainingData\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/SageMakerEstimator.py:257\u001b[0m, in \u001b[0;36mSageMakerEstimatorBase.fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fits a SageMakerModel on dataset by running a SageMaker training job.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    JavaSageMakerModel: The Model created by the training job.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_java\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/wrapper.py:78\u001b[0m, in \u001b[0;36mSageMakerJavaWrapper._call_java\u001b[0;34m(self, name, *args)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args:\n\u001b[1;32m     76\u001b[0m     java_args\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_py2j(arg))\n\u001b[0;32m---> 78\u001b[0m java_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSageMakerJavaWrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m SageMakerJavaWrapper\u001b[38;5;241m.\u001b[39m_from_java(java_value)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/wrapper.py:72\u001b[0m, in \u001b[0;36mJavaWrapper._call_java\u001b[0;34m(self, name, *args)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o430.fit.\n: java.lang.ClassNotFoundException: org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat java.base/java.lang.Class.forName0(Native Method)\n\tat java.base/java.lang.Class.forName(Class.java:398)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.internal.io.FileCommitProtocol$.instantiate(FileCommitProtocol.scala:213)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat com.amazonaws.services.sagemaker.sparksdk.internal.DataUploader.writeData(DataUploader.scala:111)\n\tat com.amazonaws.services.sagemaker.sparksdk.internal.DataUploader.uploadData(DataUploader.scala:90)\n\tat com.amazonaws.services.sagemaker.sparksdk.SageMakerEstimator.fit(SageMakerEstimator.scala:302)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "train_data_location = \"s3://sagemaker-us-east-1-548775606164/1p-notebooks-datasets/taxi/text-csv/train\"\n",
    "trainingData = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"True\")  #Trueour CSV has a header row\n",
    "    .option(\"inferSchema\", \"True\")  # Truers the data types of columns\n",
    "    .option(\"numFeatures\", \"14\")\n",
    "    .load(\"s3a://sagemaker-us-east-1-548775606164/1p-notebooks-datasets/taxi/text-csv/train\")\n",
    ")\n",
    "cn_regions = [\"cn-north-1\", \"cn-northwest-1\"]\n",
    "region = boto3.Session().region_name\n",
    "endpoint_domain = \"com.cn\" if region in cn_regions else \"com\"\n",
    "# spark._jsc.hadoopConfiguration().set(\n",
    "#     \"fs.s3a.endpoint\", \"s3.{}.amazonaws.{}\".format(region, endpoint_domain)\n",
    "# )\n",
    "# train_data = TrainingInput(\"s3://your-bucket/path/to/training/data\", content_type=\"csv\")\n",
    "\n",
    "customModel = estimator.fit(trainingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5861e569",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n",
      "Hadoop version: ['/bin/bash: line 1: hadoop: command not found']\n"
     ]
    }
   ],
   "source": [
    "# Check Spark version\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# Check Hadoop version\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hadoop:hadoop-aws:2.7.4 pyspark-shell'  # Replace with your Hadoop version if different\n",
    "hadoop_version_cmd = \"hadoop version\"\n",
    "hadoop_version_output = !{hadoop_version_cmd}\n",
    "print(\"Hadoop version:\", hadoop_version_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79f0c0c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/usr/share/aws/aws-java-sdk/*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Check for AWS SDK (Java)\n",
    "!ls -l /usr/share/aws/aws-java-sdk/*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c5af4a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.app.startTime', '1700963123268')\n",
      "('spark.repl.local.jars', 'file:///root/.ivy2/jars/org.apache.hadoop_hadoop-aws-2.7.2.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-common-2.7.2.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.2.3.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.2.3.jar,file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-1.7.4.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-annotations-2.7.2.jar,file:///root/.ivy2/jars/com.google.guava_guava-11.0.2.jar,file:///root/.ivy2/jars/commons-cli_commons-cli-1.2.jar,file:///root/.ivy2/jars/org.apache.commons_commons-math3-3.1.1.jar,file:///root/.ivy2/jars/xmlenc_xmlenc-0.52.jar,file:///root/.ivy2/jars/commons-httpclient_commons-httpclient-3.1.jar,file:///root/.ivy2/jars/commons-codec_commons-codec-1.4.jar,file:///root/.ivy2/jars/commons-io_commons-io-2.4.jar,file:///root/.ivy2/jars/commons-net_commons-net-3.1.jar,file:///root/.ivy2/jars/commons-collections_commons-collections-3.2.2.jar,file:///root/.ivy2/jars/javax.servlet_servlet-api-2.5.jar,file:///root/.ivy2/jars/org.mortbay.jetty_jetty-6.1.26.jar,file:///root/.ivy2/jars/org.mortbay.jetty_jetty-util-6.1.26.jar,file:///root/.ivy2/jars/com.sun.jersey_jersey-core-1.9.jar,file:///root/.ivy2/jars/com.sun.jersey_jersey-json-1.9.jar,file:///root/.ivy2/jars/com.sun.jersey_jersey-server-1.9.jar,file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar,file:///root/.ivy2/jars/net.java.dev.jets3t_jets3t-0.9.0.jar,file:///root/.ivy2/jars/commons-lang_commons-lang-2.6.jar,file:///root/.ivy2/jars/commons-configuration_commons-configuration-1.6.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.10.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,file:///root/.ivy2/jars/org.apache.avro_avro-1.7.4.jar,file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar,file:///root/.ivy2/jars/com.google.code.gson_gson-2.2.4.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-auth-2.7.2.jar,file:///root/.ivy2/jars/com.jcraft_jsch-0.1.42.jar,file:///root/.ivy2/jars/org.apache.curator_curator-client-2.7.1.jar,file:///root/.ivy2/jars/org.apache.curator_curator-recipes-2.7.1.jar,file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///root/.ivy2/jars/org.apache.htrace_htrace-core-3.1.0-incubating.jar,file:///root/.ivy2/jars/org.apache.zookeeper_zookeeper-3.4.6.jar,file:///root/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar,file:///root/.ivy2/jars/org.codehaus.jettison_jettison-1.1.jar,file:///root/.ivy2/jars/com.sun.xml.bind_jaxb-impl-2.2.3-1.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-jaxrs-1.9.13.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-xc-1.9.13.jar,file:///root/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.2.jar,file:///root/.ivy2/jars/javax.xml.stream_stax-api-1.0-2.jar,file:///root/.ivy2/jars/javax.activation_activation-1.1.jar,file:///root/.ivy2/jars/asm_asm-3.2.jar,file:///root/.ivy2/jars/org.apache.httpcomponents_httpclient-4.2.5.jar,file:///root/.ivy2/jars/org.apache.httpcomponents_httpcore-4.2.5.jar,file:///root/.ivy2/jars/com.jamesmurty.utils_java-xmlbuilder-0.4.jar,file:///root/.ivy2/jars/commons-digester_commons-digester-1.8.jar,file:///root/.ivy2/jars/commons-beanutils_commons-beanutils-core-1.8.0.jar,file:///root/.ivy2/jars/commons-beanutils_commons-beanutils-1.7.0.jar,file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar,file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.4.1.jar,file:///root/.ivy2/jars/org.tukaani_xz-1.0.jar,file:///root/.ivy2/jars/org.apache.directory.server_apacheds-kerberos-codec-2.0.0-M15.jar,file:///root/.ivy2/jars/org.apache.curator_curator-framework-2.7.1.jar,file:///root/.ivy2/jars/org.apache.directory.server_apacheds-i18n-2.0.0-M15.jar,file:///root/.ivy2/jars/org.apache.directory.api_api-asn1-api-1.0.0-M20.jar,file:///root/.ivy2/jars/org.apache.directory.api_api-util-1.0.0-M20.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-log4j12-1.7.10.jar,file:///root/.ivy2/jars/io.netty_netty-3.6.2.Final.jar,file:///root/.ivy2/jars/javax.servlet.jsp_jsp-api-2.1.jar,file:///root/.ivy2/jars/jline_jline-0.9.94.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.2.3.jar,file:///root/.ivy2/jars/joda-time_joda-time-2.12.5.jar')\n",
      "('spark.driver.extraClassPath', '/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/jars/aws-java-sdk-bundle-1.11.901.jar:/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/jars/aws-java-sdk-core-1.12.262.jar:/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/jars/aws-java-sdk-kms-1.12.262.jar:/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/jars/aws-java-sdk-s3-1.12.262.jar:/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sagemaker-1.12.262.jar:/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sagemakerruntime-1.12.262.jar:/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sts-1.12.262.jar:/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/jars/hadoop-aws-3.3.1.jar:/opt/conda/lib/python3.10/site-packages/sagemaker_pyspark/jars/sagemaker-spark_2.12-spark_3.3.0-1.4.5.jar')\n",
      "('spark.app.id', 'local-1700963125627')\n",
      "('spark.driver.port', '44925')\n",
      "('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.files', 'file:///root/.ivy2/jars/org.apache.hadoop_hadoop-aws-2.7.2.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-common-2.7.2.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.2.3.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.2.3.jar,file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-1.7.4.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-annotations-2.7.2.jar,file:///root/.ivy2/jars/com.google.guava_guava-11.0.2.jar,file:///root/.ivy2/jars/commons-cli_commons-cli-1.2.jar,file:///root/.ivy2/jars/org.apache.commons_commons-math3-3.1.1.jar,file:///root/.ivy2/jars/xmlenc_xmlenc-0.52.jar,file:///root/.ivy2/jars/commons-httpclient_commons-httpclient-3.1.jar,file:///root/.ivy2/jars/commons-codec_commons-codec-1.4.jar,file:///root/.ivy2/jars/commons-io_commons-io-2.4.jar,file:///root/.ivy2/jars/commons-net_commons-net-3.1.jar,file:///root/.ivy2/jars/commons-collections_commons-collections-3.2.2.jar,file:///root/.ivy2/jars/javax.servlet_servlet-api-2.5.jar,file:///root/.ivy2/jars/org.mortbay.jetty_jetty-6.1.26.jar,file:///root/.ivy2/jars/org.mortbay.jetty_jetty-util-6.1.26.jar,file:///root/.ivy2/jars/com.sun.jersey_jersey-core-1.9.jar,file:///root/.ivy2/jars/com.sun.jersey_jersey-json-1.9.jar,file:///root/.ivy2/jars/com.sun.jersey_jersey-server-1.9.jar,file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar,file:///root/.ivy2/jars/net.java.dev.jets3t_jets3t-0.9.0.jar,file:///root/.ivy2/jars/commons-lang_commons-lang-2.6.jar,file:///root/.ivy2/jars/commons-configuration_commons-configuration-1.6.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.10.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,file:///root/.ivy2/jars/org.apache.avro_avro-1.7.4.jar,file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar,file:///root/.ivy2/jars/com.google.code.gson_gson-2.2.4.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-auth-2.7.2.jar,file:///root/.ivy2/jars/com.jcraft_jsch-0.1.42.jar,file:///root/.ivy2/jars/org.apache.curator_curator-client-2.7.1.jar,file:///root/.ivy2/jars/org.apache.curator_curator-recipes-2.7.1.jar,file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///root/.ivy2/jars/org.apache.htrace_htrace-core-3.1.0-incubating.jar,file:///root/.ivy2/jars/org.apache.zookeeper_zookeeper-3.4.6.jar,file:///root/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar,file:///root/.ivy2/jars/org.codehaus.jettison_jettison-1.1.jar,file:///root/.ivy2/jars/com.sun.xml.bind_jaxb-impl-2.2.3-1.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-jaxrs-1.9.13.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-xc-1.9.13.jar,file:///root/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.2.jar,file:///root/.ivy2/jars/javax.xml.stream_stax-api-1.0-2.jar,file:///root/.ivy2/jars/javax.activation_activation-1.1.jar,file:///root/.ivy2/jars/asm_asm-3.2.jar,file:///root/.ivy2/jars/org.apache.httpcomponents_httpclient-4.2.5.jar,file:///root/.ivy2/jars/org.apache.httpcomponents_httpcore-4.2.5.jar,file:///root/.ivy2/jars/com.jamesmurty.utils_java-xmlbuilder-0.4.jar,file:///root/.ivy2/jars/commons-digester_commons-digester-1.8.jar,file:///root/.ivy2/jars/commons-beanutils_commons-beanutils-core-1.8.0.jar,file:///root/.ivy2/jars/commons-beanutils_commons-beanutils-1.7.0.jar,file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar,file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.4.1.jar,file:///root/.ivy2/jars/org.tukaani_xz-1.0.jar,file:///root/.ivy2/jars/org.apache.directory.server_apacheds-kerberos-codec-2.0.0-M15.jar,file:///root/.ivy2/jars/org.apache.curator_curator-framework-2.7.1.jar,file:///root/.ivy2/jars/org.apache.directory.server_apacheds-i18n-2.0.0-M15.jar,file:///root/.ivy2/jars/org.apache.directory.api_api-asn1-api-1.0.0-M20.jar,file:///root/.ivy2/jars/org.apache.directory.api_api-util-1.0.0-M20.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-log4j12-1.7.10.jar,file:///root/.ivy2/jars/io.netty_netty-3.6.2.Final.jar,file:///root/.ivy2/jars/javax.servlet.jsp_jsp-api-2.1.jar,file:///root/.ivy2/jars/jline_jline-0.9.94.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.2.3.jar,file:///root/.ivy2/jars/joda-time_joda-time-2.12.5.jar')\n",
      "('spark.app.initial.file.urls', 'file:///root/.ivy2/jars/org.apache.hadoop_hadoop-auth-2.7.2.jar,file:///root/.ivy2/jars/org.apache.directory.api_api-asn1-api-1.0.0-M20.jar,file:///root/.ivy2/jars/com.sun.jersey_jersey-core-1.9.jar,file:///root/.ivy2/jars/com.google.guava_guava-11.0.2.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.2.3.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-annotations-2.7.2.jar,file:///root/.ivy2/jars/com.google.code.gson_gson-2.2.4.jar,file:///root/.ivy2/jars/joda-time_joda-time-2.12.5.jar,file:///root/.ivy2/jars/jline_jline-0.9.94.jar,file:///root/.ivy2/jars/commons-codec_commons-codec-1.4.jar,file:///root/.ivy2/jars/javax.xml.stream_stax-api-1.0-2.jar,file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///root/.ivy2/jars/com.jamesmurty.utils_java-xmlbuilder-0.4.jar,file:///root/.ivy2/jars/org.apache.httpcomponents_httpclient-4.2.5.jar,file:///root/.ivy2/jars/org.apache.directory.api_api-util-1.0.0-M20.jar,file:///root/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.2.jar,file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar,file:///root/.ivy2/jars/commons-collections_commons-collections-3.2.2.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-common-2.7.2.jar,file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar,file:///root/.ivy2/jars/commons-cli_commons-cli-1.2.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-xc-1.9.13.jar,file:///root/.ivy2/jars/org.tukaani_xz-1.0.jar,file:///root/.ivy2/jars/org.apache.httpcomponents_httpcore-4.2.5.jar,file:///root/.ivy2/jars/org.apache.curator_curator-recipes-2.7.1.jar,file:///root/.ivy2/jars/commons-beanutils_commons-beanutils-core-1.8.0.jar,file:///root/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar,file:///root/.ivy2/jars/commons-lang_commons-lang-2.6.jar,file:///root/.ivy2/jars/commons-beanutils_commons-beanutils-1.7.0.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.2.3.jar,file:///root/.ivy2/jars/io.netty_netty-3.6.2.Final.jar,file:///root/.ivy2/jars/com.jcraft_jsch-0.1.42.jar,file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.4.1.jar,file:///root/.ivy2/jars/org.mortbay.jetty_jetty-6.1.26.jar,file:///root/.ivy2/jars/org.mortbay.jetty_jetty-util-6.1.26.jar,file:///root/.ivy2/jars/net.java.dev.jets3t_jets3t-0.9.0.jar,file:///root/.ivy2/jars/asm_asm-3.2.jar,file:///root/.ivy2/jars/org.apache.avro_avro-1.7.4.jar,file:///root/.ivy2/jars/javax.servlet_servlet-api-2.5.jar,file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-1.7.4.jar,file:///root/.ivy2/jars/org.codehaus.jettison_jettison-1.1.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.10.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-aws-2.7.2.jar,file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar,file:///root/.ivy2/jars/org.apache.htrace_htrace-core-3.1.0-incubating.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,file:///root/.ivy2/jars/commons-httpclient_commons-httpclient-3.1.jar,file:///root/.ivy2/jars/com.sun.jersey_jersey-server-1.9.jar,file:///root/.ivy2/jars/org.apache.zookeeper_zookeeper-3.4.6.jar,file:///root/.ivy2/jars/commons-io_commons-io-2.4.jar,file:///root/.ivy2/jars/com.sun.jersey_jersey-json-1.9.jar,file:///root/.ivy2/jars/commons-configuration_commons-configuration-1.6.jar,file:///root/.ivy2/jars/commons-digester_commons-digester-1.8.jar,file:///root/.ivy2/jars/org.apache.directory.server_apacheds-kerberos-codec-2.0.0-M15.jar,file:///root/.ivy2/jars/com.sun.xml.bind_jaxb-impl-2.2.3-1.jar,file:///root/.ivy2/jars/org.apache.curator_curator-client-2.7.1.jar,file:///root/.ivy2/jars/org.apache.directory.server_apacheds-i18n-2.0.0-M15.jar,file:///root/.ivy2/jars/xmlenc_xmlenc-0.52.jar,file:///root/.ivy2/jars/commons-net_commons-net-3.1.jar,file:///root/.ivy2/jars/org.apache.commons_commons-math3-3.1.1.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,file:///root/.ivy2/jars/javax.servlet.jsp_jsp-api-2.1.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.2.3.jar,file:///root/.ivy2/jars/org.apache.curator_curator-framework-2.7.1.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-jaxrs-1.9.13.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-log4j12-1.7.10.jar,file:///root/.ivy2/jars/javax.activation_activation-1.1.jar')\n",
      "('fs.s3a.access.key', 'AKIAX7RMTG6KERGGBYZW')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.app.submitTime', '1700963122797')\n",
      "('spark.app.name', 'schema_test')\n",
      "('spark.jars', 'file:///root/.ivy2/jars/org.apache.hadoop_hadoop-aws-2.7.2.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-common-2.7.2.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.2.3.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.2.3.jar,file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-1.7.4.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-annotations-2.7.2.jar,file:///root/.ivy2/jars/com.google.guava_guava-11.0.2.jar,file:///root/.ivy2/jars/commons-cli_commons-cli-1.2.jar,file:///root/.ivy2/jars/org.apache.commons_commons-math3-3.1.1.jar,file:///root/.ivy2/jars/xmlenc_xmlenc-0.52.jar,file:///root/.ivy2/jars/commons-httpclient_commons-httpclient-3.1.jar,file:///root/.ivy2/jars/commons-codec_commons-codec-1.4.jar,file:///root/.ivy2/jars/commons-io_commons-io-2.4.jar,file:///root/.ivy2/jars/commons-net_commons-net-3.1.jar,file:///root/.ivy2/jars/commons-collections_commons-collections-3.2.2.jar,file:///root/.ivy2/jars/javax.servlet_servlet-api-2.5.jar,file:///root/.ivy2/jars/org.mortbay.jetty_jetty-6.1.26.jar,file:///root/.ivy2/jars/org.mortbay.jetty_jetty-util-6.1.26.jar,file:///root/.ivy2/jars/com.sun.jersey_jersey-core-1.9.jar,file:///root/.ivy2/jars/com.sun.jersey_jersey-json-1.9.jar,file:///root/.ivy2/jars/com.sun.jersey_jersey-server-1.9.jar,file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar,file:///root/.ivy2/jars/net.java.dev.jets3t_jets3t-0.9.0.jar,file:///root/.ivy2/jars/commons-lang_commons-lang-2.6.jar,file:///root/.ivy2/jars/commons-configuration_commons-configuration-1.6.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.10.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,file:///root/.ivy2/jars/org.apache.avro_avro-1.7.4.jar,file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar,file:///root/.ivy2/jars/com.google.code.gson_gson-2.2.4.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-auth-2.7.2.jar,file:///root/.ivy2/jars/com.jcraft_jsch-0.1.42.jar,file:///root/.ivy2/jars/org.apache.curator_curator-client-2.7.1.jar,file:///root/.ivy2/jars/org.apache.curator_curator-recipes-2.7.1.jar,file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///root/.ivy2/jars/org.apache.htrace_htrace-core-3.1.0-incubating.jar,file:///root/.ivy2/jars/org.apache.zookeeper_zookeeper-3.4.6.jar,file:///root/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar,file:///root/.ivy2/jars/org.codehaus.jettison_jettison-1.1.jar,file:///root/.ivy2/jars/com.sun.xml.bind_jaxb-impl-2.2.3-1.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-jaxrs-1.9.13.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-xc-1.9.13.jar,file:///root/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.2.jar,file:///root/.ivy2/jars/javax.xml.stream_stax-api-1.0-2.jar,file:///root/.ivy2/jars/javax.activation_activation-1.1.jar,file:///root/.ivy2/jars/asm_asm-3.2.jar,file:///root/.ivy2/jars/org.apache.httpcomponents_httpclient-4.2.5.jar,file:///root/.ivy2/jars/org.apache.httpcomponents_httpcore-4.2.5.jar,file:///root/.ivy2/jars/com.jamesmurty.utils_java-xmlbuilder-0.4.jar,file:///root/.ivy2/jars/commons-digester_commons-digester-1.8.jar,file:///root/.ivy2/jars/commons-beanutils_commons-beanutils-core-1.8.0.jar,file:///root/.ivy2/jars/commons-beanutils_commons-beanutils-1.7.0.jar,file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar,file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.4.1.jar,file:///root/.ivy2/jars/org.tukaani_xz-1.0.jar,file:///root/.ivy2/jars/org.apache.directory.server_apacheds-kerberos-codec-2.0.0-M15.jar,file:///root/.ivy2/jars/org.apache.curator_curator-framework-2.7.1.jar,file:///root/.ivy2/jars/org.apache.directory.server_apacheds-i18n-2.0.0-M15.jar,file:///root/.ivy2/jars/org.apache.directory.api_api-asn1-api-1.0.0-M20.jar,file:///root/.ivy2/jars/org.apache.directory.api_api-util-1.0.0-M20.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-log4j12-1.7.10.jar,file:///root/.ivy2/jars/io.netty_netty-3.6.2.Final.jar,file:///root/.ivy2/jars/javax.servlet.jsp_jsp-api-2.1.jar,file:///root/.ivy2/jars/jline_jline-0.9.94.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.2.3.jar,file:///root/.ivy2/jars/joda-time_joda-time-2.12.5.jar')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.sql.warehouse.dir', 'file:/root/spark-warehouse')\n",
      "('spark.app.initial.jar.urls', 'spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.httpcomponents_httpclient-4.2.5.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/net.java.dev.jets3t_jets3t-0.9.0.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.mortbay.jetty_jetty-util-6.1.26.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/commons-net_commons-net-3.1.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.codehaus.jackson_jackson-jaxrs-1.9.13.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/javax.servlet.jsp_jsp-api-2.1.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/commons-lang_commons-lang-2.6.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/commons-collections_commons-collections-3.2.2.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.mortbay.jetty_jetty-6.1.26.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.zookeeper_zookeeper-3.4.6.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.xerial.snappy_snappy-java-1.0.4.1.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/com.google.protobuf_protobuf-java-2.5.0.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/xmlenc_xmlenc-0.52.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/log4j_log4j-1.2.17.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/com.sun.xml.bind_jaxb-impl-2.2.3-1.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/jline_jline-0.9.94.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/com.fasterxml.jackson.core_jackson-databind-2.2.3.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.curator_curator-recipes-2.7.1.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/com.jcraft_jsch-0.1.42.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/joda-time_joda-time-2.12.5.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/commons-cli_commons-cli-1.2.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.directory.server_apacheds-kerberos-codec-2.0.0-M15.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.directory.api_api-asn1-api-1.0.0-M20.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/javax.xml.bind_jaxb-api-2.2.2.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/com.thoughtworks.paranamer_paranamer-2.3.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/javax.servlet_servlet-api-2.5.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.curator_curator-client-2.7.1.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.commons_commons-compress-1.4.1.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/com.sun.jersey_jersey-json-1.9.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.hadoop_hadoop-auth-2.7.2.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.htrace_htrace-core-3.1.0-incubating.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.tukaani_xz-1.0.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/com.fasterxml.jackson.core_jackson-core-2.2.3.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/javax.xml.stream_stax-api-1.0-2.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.codehaus.jackson_jackson-xc-1.9.13.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/commons-httpclient_commons-httpclient-3.1.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/com.google.code.gson_gson-2.2.4.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/io.netty_netty-3.6.2.Final.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/asm_asm-3.2.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.curator_curator-framework-2.7.1.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.avro_avro-1.7.4.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/javax.activation_activation-1.1.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.directory.api_api-util-1.0.0-M20.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.slf4j_slf4j-api-1.7.10.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/com.google.code.findbugs_jsr305-3.0.0.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/com.sun.jersey_jersey-core-1.9.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.commons_commons-math3-3.1.1.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.hadoop_hadoop-common-2.7.2.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.hadoop_hadoop-annotations-2.7.2.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/com.amazonaws_aws-java-sdk-1.7.4.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.hadoop_hadoop-aws-2.7.2.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/com.fasterxml.jackson.core_jackson-annotations-2.2.3.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/commons-beanutils_commons-beanutils-core-1.8.0.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/commons-configuration_commons-configuration-1.6.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/com.google.guava_guava-11.0.2.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.directory.server_apacheds-i18n-2.0.0-M15.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/commons-codec_commons-codec-1.4.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.apache.httpcomponents_httpcore-4.2.5.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/commons-digester_commons-digester-1.8.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/com.jamesmurty.utils_java-xmlbuilder-0.4.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.slf4j_slf4j-log4j12-1.7.10.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/commons-io_commons-io-2.4.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/commons-beanutils_commons-beanutils-1.7.0.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/com.sun.jersey_jersey-server-1.9.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/org.codehaus.jettison_jettison-1.1.jar,spark://sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c:44925/jars/commons-logging_commons-logging-1.1.3.jar')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('fs.s3a.secret.key', 'I0z79iqo6CRQDDNu35/3f2Y7RwsCZ+QqQ1LvMjCZ')\n",
      "('spark.driver.host', 'sagemaker-data-scienc-ml-t3-medium-ccb588b5efaf671be41927273f0c')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.submit.pyFiles', '/root/.ivy2/jars/org.apache.hadoop_hadoop-aws-2.7.2.jar,/root/.ivy2/jars/org.apache.hadoop_hadoop-common-2.7.2.jar,/root/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.2.3.jar,/root/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.2.3.jar,/root/.ivy2/jars/com.amazonaws_aws-java-sdk-1.7.4.jar,/root/.ivy2/jars/org.apache.hadoop_hadoop-annotations-2.7.2.jar,/root/.ivy2/jars/com.google.guava_guava-11.0.2.jar,/root/.ivy2/jars/commons-cli_commons-cli-1.2.jar,/root/.ivy2/jars/org.apache.commons_commons-math3-3.1.1.jar,/root/.ivy2/jars/xmlenc_xmlenc-0.52.jar,/root/.ivy2/jars/commons-httpclient_commons-httpclient-3.1.jar,/root/.ivy2/jars/commons-codec_commons-codec-1.4.jar,/root/.ivy2/jars/commons-io_commons-io-2.4.jar,/root/.ivy2/jars/commons-net_commons-net-3.1.jar,/root/.ivy2/jars/commons-collections_commons-collections-3.2.2.jar,/root/.ivy2/jars/javax.servlet_servlet-api-2.5.jar,/root/.ivy2/jars/org.mortbay.jetty_jetty-6.1.26.jar,/root/.ivy2/jars/org.mortbay.jetty_jetty-util-6.1.26.jar,/root/.ivy2/jars/com.sun.jersey_jersey-core-1.9.jar,/root/.ivy2/jars/com.sun.jersey_jersey-json-1.9.jar,/root/.ivy2/jars/com.sun.jersey_jersey-server-1.9.jar,/root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,/root/.ivy2/jars/log4j_log4j-1.2.17.jar,/root/.ivy2/jars/net.java.dev.jets3t_jets3t-0.9.0.jar,/root/.ivy2/jars/commons-lang_commons-lang-2.6.jar,/root/.ivy2/jars/commons-configuration_commons-configuration-1.6.jar,/root/.ivy2/jars/org.slf4j_slf4j-api-1.7.10.jar,/root/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,/root/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,/root/.ivy2/jars/org.apache.avro_avro-1.7.4.jar,/root/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar,/root/.ivy2/jars/com.google.code.gson_gson-2.2.4.jar,/root/.ivy2/jars/org.apache.hadoop_hadoop-auth-2.7.2.jar,/root/.ivy2/jars/com.jcraft_jsch-0.1.42.jar,/root/.ivy2/jars/org.apache.curator_curator-client-2.7.1.jar,/root/.ivy2/jars/org.apache.curator_curator-recipes-2.7.1.jar,/root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,/root/.ivy2/jars/org.apache.htrace_htrace-core-3.1.0-incubating.jar,/root/.ivy2/jars/org.apache.zookeeper_zookeeper-3.4.6.jar,/root/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar,/root/.ivy2/jars/org.codehaus.jettison_jettison-1.1.jar,/root/.ivy2/jars/com.sun.xml.bind_jaxb-impl-2.2.3-1.jar,/root/.ivy2/jars/org.codehaus.jackson_jackson-jaxrs-1.9.13.jar,/root/.ivy2/jars/org.codehaus.jackson_jackson-xc-1.9.13.jar,/root/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.2.jar,/root/.ivy2/jars/javax.xml.stream_stax-api-1.0-2.jar,/root/.ivy2/jars/javax.activation_activation-1.1.jar,/root/.ivy2/jars/asm_asm-3.2.jar,/root/.ivy2/jars/org.apache.httpcomponents_httpclient-4.2.5.jar,/root/.ivy2/jars/org.apache.httpcomponents_httpcore-4.2.5.jar,/root/.ivy2/jars/com.jamesmurty.utils_java-xmlbuilder-0.4.jar,/root/.ivy2/jars/commons-digester_commons-digester-1.8.jar,/root/.ivy2/jars/commons-beanutils_commons-beanutils-core-1.8.0.jar,/root/.ivy2/jars/commons-beanutils_commons-beanutils-1.7.0.jar,/root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar,/root/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.4.1.jar,/root/.ivy2/jars/org.tukaani_xz-1.0.jar,/root/.ivy2/jars/org.apache.directory.server_apacheds-kerberos-codec-2.0.0-M15.jar,/root/.ivy2/jars/org.apache.curator_curator-framework-2.7.1.jar,/root/.ivy2/jars/org.apache.directory.server_apacheds-i18n-2.0.0-M15.jar,/root/.ivy2/jars/org.apache.directory.api_api-asn1-api-1.0.0-M20.jar,/root/.ivy2/jars/org.apache.directory.api_api-util-1.0.0-M20.jar,/root/.ivy2/jars/org.slf4j_slf4j-log4j12-1.7.10.jar,/root/.ivy2/jars/io.netty_netty-3.6.2.Final.jar,/root/.ivy2/jars/javax.servlet.jsp_jsp-api-2.1.jar,/root/.ivy2/jars/jline_jline-0.9.94.jar,/root/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.2.3.jar,/root/.ivy2/jars/joda-time_joda-time-2.12.5.jar')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n"
     ]
    }
   ],
   "source": [
    "# Show Spark configurations\n",
    "for conf in spark.sparkContext.getConf().getAll():\n",
    "    print(conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "feec9bee-2415-4679-9961-2182f3b3a2b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameter validation failed:\n",
      "Invalid bucket name \"s3a:\": Bucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN matching the regex \"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[/:][a-zA-Z0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[/:][a-zA-Z0-9\\-]{1,63}[/:]accesspoint[/:][a-zA-Z0-9\\-]{1,63}$\"\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3a://sagemaker-us-east-1-548775606164/1p-notebooks-datasets/taxi/text-csv/train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5dfa67-dbe7-4cfe-9e65-573857076c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
